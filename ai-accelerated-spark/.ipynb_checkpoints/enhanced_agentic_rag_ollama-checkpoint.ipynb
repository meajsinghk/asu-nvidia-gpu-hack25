{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f625337",
   "metadata": {},
   "source": [
    "# Agentic RAG with Local Ollama Model\n",
    "This notebook demonstrates how to build a Retrieval-Augmented Generation (RAG) agent using LangGraph, LangChain, and a local  model run via Ollama.\n",
    "\n",
    "Adapted from: https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_agentic_rag/\n",
    "\n",
    "## Materials\n",
    "This notebook and all materials referenced here can be found on Sol `/data/sse/ai-accelerated-spark`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbc8378-b454-48d8-a95c-563521334562",
   "metadata": {},
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cfaace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage\n",
    "from langchain_core.tools import Tool\n",
    "from langgraph.graph import Graph\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "import os\n",
    "\n",
    "os.environ[\"USER_AGENT\"] = \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/109.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59114acf",
   "metadata": {},
   "source": [
    "## 2. Preprocess documents\n",
    "### 2.1. Fetch documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bf7599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders import NotebookLoader\n",
    "from langchain.docstore.document import Document\n",
    "import os\n",
    "\n",
    "# External URLs for GPU acceleration knowledge\n",
    "urls = [\n",
    "    \"https://medium.com/cupy-team/announcing-cupy-v13-66979ee7fab0\",\n",
    "    \"https://www.unum.cloud/blog/2022-01-26-cupy\",\n",
    "    \"https://medium.com/rapids-ai/easy-cpu-gpu-arrays-and-dataframes-run-your-dask-code-where-youd-like-e349d92351d\"\n",
    "]\n",
    "\n",
    "docs = []\n",
    "for url in urls:\n",
    "    try:\n",
    "        loaded_docs = WebBaseLoader(url).load()\n",
    "        docs.extend(loaded_docs)\n",
    "        print(f\"âœ… Loaded web content from {url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Could not load {url}: {str(e)}\")\n",
    "\n",
    "# Load local notebook content for comprehensive GPU acceleration examples\n",
    "notebook_dir = \"../python_notebooks\"\n",
    "notebook_files = [\n",
    "    \"notebook-1-cupy.ipynb\",\n",
    "    \"notebook-2-rapids-cudf.ipynb\", \n",
    "    \"notebook-3-rapids-cuml.ipynb\",\n",
    "    \"notebook-4-warp.ipynb\"\n",
    "]\n",
    "\n",
    "# Load notebook content if available\n",
    "for nb_file in notebook_files:\n",
    "    nb_path = os.path.join(notebook_dir, nb_file)\n",
    "    if os.path.exists(nb_path):\n",
    "        try:\n",
    "            nb_loader = NotebookLoader(nb_path, include_outputs=True, max_output_length=1000)\n",
    "            nb_docs = nb_loader.load()\n",
    "            docs.extend(nb_docs)\n",
    "            print(f\"âœ… Loaded {nb_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Could not load {nb_file}: {str(e)}\")\n",
    "    else:\n",
    "        print(f\"âŒ Notebook not found: {nb_path}\")\n",
    "\n",
    "# Add curated GPU acceleration content\n",
    "gpu_acceleration_content = \"\"\"\n",
    "# GPU Acceleration with NVIDIA Rapids\n",
    "\n",
    "## CuPy Performance Patterns\n",
    "- Matrix operations show 5-50x speedup on GPU vs CPU\n",
    "- Best performance for arrays > 1M elements\n",
    "- Memory bandwidth is often the bottleneck\n",
    "- Use .astype() to ensure optimal data types (float32)\n",
    "- Kernel launch overhead affects small operations\n",
    "\n",
    "## cuDF Performance Benefits  \n",
    "- DataFrame operations can achieve 10-100x speedup\n",
    "- GroupBy operations scale excellently on GPU\n",
    "- String operations benefit significantly from GPU parallelization\n",
    "- Best for datasets > 100K rows\n",
    "- Memory management is crucial for large datasets\n",
    "\n",
    "## cuML Machine Learning Acceleration\n",
    "- K-Means clustering: 10-50x speedup typical\n",
    "- Random Forest: 5-25x speedup\n",
    "- Logistic Regression: 3-15x speedup\n",
    "- UMAP/t-SNE: 10-100x speedup for dimensionality reduction\n",
    "\n",
    "## Best Practices for GPU Acceleration\n",
    "1. Keep data on GPU between operations\n",
    "2. Use appropriate data types (prefer float32 over float64)\n",
    "3. Batch operations to amortize kernel launch overhead\n",
    "4. Profile memory usage and optimize transfers\n",
    "5. Use @cupy.fuse for element-wise operations\n",
    "6. Consider problem size - GPU overhead for small data\n",
    "\n",
    "## When NOT to use GPU\n",
    "- Very small datasets (< 10K elements)\n",
    "- Sequential algorithms that don't parallelize\n",
    "- Frequent CPU-GPU memory transfers\n",
    "- Operations dominated by I/O\n",
    "\"\"\"\n",
    "\n",
    "# Add curated content as a Document object\n",
    "docs.append(Document(page_content=gpu_acceleration_content, metadata={\"source\": \"curated_gpu_guide\"}))\n",
    "\n",
    "print(f\"ðŸ“š Total documents loaded: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6bc96c-c2c6-4ca8-af17-1a0dc06d46cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].page_content.strip()[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dcb754-10f3-4a16-b522-c1ec4cbd812b",
   "metadata": {},
   "source": [
    "### 2.2. Split the fetched documents into smaller chunks for indexing into the vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97649da8-f2bc-49fd-be94-009b59a1a5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "docs_list = docs  # docs is already a flat list of Document objects\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=100, chunk_overlap=50\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cad859-5123-4811-b53a-a58ee9be9232",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_splits[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbf0613",
   "metadata": {},
   "source": [
    "## 3.Create a retriever tool\n",
    "### 3.1. Use an in-memory vector store and all-MiniLM-L6-V2 embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb39231",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "vectorstore = InMemoryVectorStore.from_documents(\n",
    "    documents = doc_splits, embedding = embedding_model\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcb57e5-dcb5-4387-8327-5756c2df4962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use ChromaDB for persistent vectorstore\n",
    "# https://python.langchain.com/docs/integrations/vectorstores/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f517c44-2e14-4614-9290-8d75e7faef4f",
   "metadata": {},
   "source": [
    "### 3.2. Create a retriever tool using LangChain's prebuild `create_retriever_tool`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5278231-c5eb-40ca-94f5-8a74d99a1b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retrieve_python_gpu_acceleration\",\n",
    "    \"Search and return information about accelerating Python code using the GPU with RAPIDS and CuPy.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fcbee6-e9d6-466f-9c7a-303ca012b469",
   "metadata": {},
   "source": [
    "### 3.3. Test the tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba4b915-9cfc-4e71-9ea2-6060cde12d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_tool.invoke({\"query\": \"How can I create a CuPy-backed Dask array for random data?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29127ddb-5909-4809-8525-3204ef2d08cf",
   "metadata": {},
   "source": [
    "## 4. Generate query\n",
    "### 4.1. Load local LLM\n",
    "\n",
    "Start ollama using the terminal:\n",
    "```bash\n",
    "module load ollama/0.9.0\n",
    "export OLLAMA_MODELS=/data/datasets/community/ollama\n",
    "ollama-start\n",
    "```\n",
    "\n",
    "Check the available list of models using `ollama list`. Let me know via Slack if you would like to use and test other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4504022-deee-4190-b681-6dfa9ba5e758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "import socket\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "host_node = socket.gethostname()\n",
    "llm_model = init_chat_model(\"ollama:qwen3:14b\", temperature=0, base_url=f\"http://vpatel69@{host_node}:11434/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972da38e-7439-4d43-8fc6-6aafcdf00069",
   "metadata": {},
   "source": [
    "### 4.2. Build a `generate_query_or_respond` node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ecc05d-868d-4228-bc67-bd8ed7b65a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "import re\n",
    "\n",
    "def generate_query_or_respond(state: MessagesState):\n",
    "    \"\"\"Call the model to generate a response based on the current state. Given\n",
    "    the question, it will decide to retrieve using the retriever tool, or simply respond to the user.\n",
    "    \"\"\"\n",
    "    response = (\n",
    "        llm_model\n",
    "        .bind_tools([retriever_tool]).invoke(state[\"messages\"])\n",
    "    )\n",
    "    # remove thinking text\n",
    "    content = re.sub(r\"<think>.*</think>\", \"\", response.content, flags=re.DOTALL).strip()\n",
    "    response.content = content\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0091934-a78a-4a46-bdd6-24d62eae69ed",
   "metadata": {},
   "source": [
    "### 4.3. Try a random input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbc5702-ff03-4934-b664-3d6a8edca463",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\"messages\": [{\"role\": \"user\", \"content\": \"Hello! What is the color of the sky?\"}]}\n",
    "generate_query_or_respond(input)[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50e1018-8b5c-405e-ab80-2bfc79e9a7ab",
   "metadata": {},
   "source": [
    "### 4.4. Try semantic search question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934dd1bc-fa2c-414c-80f9-cb94b93ffde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How can I create a CuPy-backed Dask array for random data?\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "generate_query_or_respond(input)[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddaf2a9-6dac-4e24-a77d-2ee588ac6eed",
   "metadata": {},
   "source": [
    "## 5. Grade documents\n",
    "### 5.1. Add conditional edge `grade_documents` to determine the relevance of retrieved documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6294ac9-5927-4f26-ac89-046829bcdfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "GRADE_PROMPT = (\n",
    "    \"You are a grader assessing relevance of a retrieved document to a user question. \\n \"\n",
    "    \"Here is the retrieved document: \\n\\n {context} \\n\\n\"\n",
    "    \"Here is the user question: {question} \\n\"\n",
    "    \"If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\"\n",
    "    \"Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\n",
    ")\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Grade documents using a binary score for relevance check.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Relevance score: 'yes' if relevant, or 'no' if not relevant\"\n",
    "    )\n",
    "\n",
    "\n",
    "def grade_documents(\n",
    "    state: MessagesState,\n",
    ") -> Literal[\"generate_answer\", \"rewrite_question\"]:\n",
    "    \"\"\"Determine whether the retrieved documents are relevant to the question.\"\"\"\n",
    "    question = state[\"messages\"][0].content\n",
    "    context = state[\"messages\"][-1].content\n",
    "    \n",
    "\n",
    "    prompt = GRADE_PROMPT.format(question=question, context=context)\n",
    "    response = (\n",
    "        llm_model\n",
    "        .with_structured_output(GradeDocuments).invoke(\n",
    "            [{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "    )\n",
    "    score = response.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        return \"generate_answer\"\n",
    "    else:\n",
    "        return \"rewrite_question\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a918ef9-e933-4f0c-83ce-75d02585a241",
   "metadata": {},
   "source": [
    "### 5.2. Try with irrelevant documents in the tool response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00be481b-9484-40db-98c5-f227129cafd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import convert_to_messages\n",
    "\n",
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"How can I create a CuPy-backed Dask array for random data?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_python_gpu_acceleration\",\n",
    "                        \"args\": {\"query\": \"creating CuPy-backed Dask arrays for random data\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\"role\": \"tool\", \"content\": \"meow\", \"tool_call_id\": \"1\"},\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "grade_documents(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf4e35d-63e6-4658-9ade-c6e835c6e39e",
   "metadata": {},
   "source": [
    "### 5.3. Try with relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7119fa-55d5-4179-8bcc-661a68e100b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"How can I create a CuPy-backed Dask array for random data?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_python_gpu_acceleration\",\n",
    "                        \"args\": {\"query\": \"creating CuPy-backed Dask arrays for random data\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": 'Now, we can leverage the array.backend configuration to create a CuPy-backed Dask array for random data:>>> with dask.config.set({â€œarray.backendâ€: â€œcupyâ€}):â€¦    darr = da.random.randint(0, 3, size=(10, 20), chunks=(2, 5)) #\\n\\n= rs.randint(0, 3, size=(10, 20), chunks=(2, 5))>>> darrdask.array<randint, shape=(10, 20), dtype=int64, chunksize=(2, 5), \\\\chunktype=cupy.ndarray>Now, we can leverage the array.backend configuration to create a CuPy-backed Dask array for random data:>>> with\\n\\nfor random array creation.',\n",
    "                \"tool_call_id\": \"1\",\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "grade_documents(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6520541-4db0-4db2-839d-66d74494644b",
   "metadata": {},
   "source": [
    "## 6.\n",
    "### 6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199cb600-4cad-4e9a-80fc-93746877b3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "REWRITE_PROMPT = (\n",
    "    \"Look at the input and try to reason about the underlying semantic intent / meaning.\\n\"\n",
    "    \"Here is the initial question:\"\n",
    "    \"\\n ------- \\n\"\n",
    "    \"{question}\"\n",
    "    \"\\n ------- \\n\"\n",
    "    \"Formulate an improved question:\"\n",
    ")\n",
    "\n",
    "\n",
    "def rewrite_question(state: MessagesState):\n",
    "    \"\"\"Rewrite the original user question.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    prompt = REWRITE_PROMPT.format(question=question)\n",
    "    response = llm_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "    # remove thinking text\n",
    "    content = re.sub(r\"<think>.*</think>\", \"\", response.content, flags=re.DOTALL).strip()\n",
    "    response.content = content\n",
    "    return {\"messages\": [{\"role\": \"user\", \"content\": response.content}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f025fedf-4e70-443f-acae-c384b7163d8f",
   "metadata": {},
   "source": [
    "### 6.2 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114f4730-0d74-4172-aaf8-9dc8c55a60a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"How can I create a CuPy-backed Dask array for random data?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_python_gpu_acceleration\",\n",
    "                        \"args\": {\"query\": \"creating CuPy-backed Dask arrays for random data\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\"role\": \"tool\", \"content\": \"meow\", \"tool_call_id\": \"1\"},\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "response = rewrite_question(input)\n",
    "print(response[\"messages\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a0771d-200e-4a1c-9da3-e8a4e82fb480",
   "metadata": {},
   "source": [
    "## 7. Generate an answer\n",
    "### 7.1. Build `generate_answer` node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3472875-cfd6-4b3f-8b39-7a1139bd7ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_PROMPT = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer the question. \"\n",
    "    \"If you don't know the answer, just say that you don't know. \"\n",
    "    \"Use three sentences maximum and keep the answer concise.\\n\"\n",
    "    \"Question: {question} \\n\"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "\n",
    "\n",
    "def generate_answer(state: MessagesState):\n",
    "    \"\"\"Generate an answer.\"\"\"\n",
    "    question = state[\"messages\"][0].content\n",
    "    context = state[\"messages\"][-1].content\n",
    "    prompt = GENERATE_PROMPT.format(question=question, context=context)\n",
    "    response = llm_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "    # remove thinking text\n",
    "    content = re.sub(r\"<think>.*</think>\", \"\", response.content, flags=re.DOTALL).strip()\n",
    "    response.content = content\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cd1250-820e-40f8-ad58-c85bca8e2c2e",
   "metadata": {},
   "source": [
    "## 7.2 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d74558-2403-4c2c-a50c-944cd63dc501",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"How can I create a CuPy-backed Dask array for random data?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_python_gpu_acceleration\",\n",
    "                        \"args\": {\"query\": \"creating CuPy-backed Dask arrays for random data\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": 'Now, we can leverage the array.backend configuration to create a CuPy-backed Dask array for random data:>>> with dask.config.set({â€œarray.backendâ€: â€œcupyâ€}):â€¦    darr = da.random.randint(0, 3, size=(10, 20), chunks=(2, 5)) #\\n\\n= rs.randint(0, 3, size=(10, 20), chunks=(2, 5))>>> darrdask.array<randint, shape=(10, 20), dtype=int64, chunksize=(2, 5), \\\\chunktype=cupy.ndarray>Now, we can leverage the array.backend configuration to create a CuPy-backed Dask array for random data:>>> with\\n\\nfor random array creation.',\n",
    "                \"tool_call_id\": \"1\",\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "response = generate_answer(input)\n",
    "response[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb5fc5f-b09c-42b1-88d9-f2b6784b2eda",
   "metadata": {},
   "source": [
    "## 8. Assemble the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bf2038-5fa5-4e22-a208-4b1167e1cede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Define the nodes we will cycle between\n",
    "workflow.add_node(generate_query_or_respond)\n",
    "workflow.add_node(\"retrieve\", ToolNode([retriever_tool]))\n",
    "workflow.add_node(rewrite_question)\n",
    "workflow.add_node(generate_answer)\n",
    "\n",
    "workflow.add_edge(START, \"generate_query_or_respond\")\n",
    "\n",
    "# Decide whether to retrieve\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate_query_or_respond\",\n",
    "    # Assess LLM decision (call `retriever_tool` tool or respond to the user)\n",
    "    tools_condition,\n",
    "    {\n",
    "        # Translate the condition outputs to nodes in our graph\n",
    "        \"tools\": \"retrieve\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Edges taken after the `action` node is called.\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    # Assess agent decision\n",
    "    grade_documents,\n",
    ")\n",
    "workflow.add_edge(\"generate_answer\", END)\n",
    "workflow.add_edge(\"rewrite_question\", \"generate_query_or_respond\")\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44857e0-4ce3-4a02-ba3b-ade019288936",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cc9c8a-4a28-49dc-ae10-ca7b8eb42e43",
   "metadata": {},
   "source": [
    "## 9. Run the agentic RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d926a1-4f35-42ed-bd08-1027c975644b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"How can I create a CuPy-backed Dask array for random data?\",\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "):\n",
    "    for node, update in chunk.items():\n",
    "        print(\"Update from node\", node)\n",
    "        update[\"messages\"][-1].pretty_print()\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9131337e-17ea-49e6-a9aa-b0c4798ddfe1",
   "metadata": {},
   "source": [
    "## 10. Graphic User Interface using Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d135bb8-88ae-4cc1-9a5a-5ffde1232826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def ask_graph(user_input, chat_history):\n",
    "    result = graph.invoke({\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    response = result[\"messages\"][-1].content\n",
    "\n",
    "    if not chat_history:\n",
    "        response = [{\"role\": \"user\", \"content\": user_input}, {\"role\": \"assistant\", \"content\": response}]\n",
    "    else:\n",
    "        response = chat_history + [{\"role\": \"user\", \"content\": user_input}, {\"role\": \"assistant\", \"content\": response}]\n",
    "\n",
    "    return \"\", response\n",
    "\n",
    "def clear_conversation():\n",
    "    return \"\", \"\"\n",
    "\n",
    "with gr.Blocks(fill_height=True, fill_width=True) as demo:\n",
    "    gr.Markdown(\"### Agentic RAG\")\n",
    "\n",
    "    with gr.Column():\n",
    "\n",
    "        with gr.Row():\n",
    "            chatbot = gr.Chatbot(height=350, type=\"messages\")\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=4):\n",
    "                query_input = gr.Textbox(\n",
    "                    label=\"Enter text here\", placeholder=\"Ask something...\", lines=1\n",
    "                    )\n",
    "            with gr.Column(scale=1):\n",
    "                with gr.Row():\n",
    "                    submit_btn = gr.Button(\"â¬†\")\n",
    "                # ðŸ§¹ Clear button\n",
    "                with gr.Row():\n",
    "                    clear_btn = gr.Button(\"ðŸ§¹ Clear Conversation\")\n",
    "\n",
    "        submit_btn.click(\n",
    "            fn=ask_graph,\n",
    "            inputs=[query_input, chatbot],\n",
    "            outputs=[query_input, chatbot],\n",
    "        )\n",
    "\n",
    "        query_input.submit(\n",
    "            fn=ask_graph,\n",
    "            inputs=[query_input, chatbot],\n",
    "            outputs=[query_input, chatbot],\n",
    "        )\n",
    "\n",
    "        clear_btn.click(\n",
    "            fn=clear_conversation,\n",
    "            outputs=[query_input, chatbot],\n",
    "        )\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe47fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dependencies for Enhanced GPU Mentor\n",
    "import sys\n",
    "\n",
    "required_packages = {\n",
    "    'plotly': 'plotly',\n",
    "    'gradio': 'gradio', \n",
    "    'pandas': 'pandas',\n",
    "    'numpy': 'numpy'\n",
    "}\n",
    "\n",
    "missing_packages = []\n",
    "\n",
    "for package, import_name in required_packages.items():\n",
    "    try:\n",
    "        __import__(import_name)\n",
    "        print(f\"âœ… {package} - Available\")\n",
    "    except ImportError:\n",
    "        print(f\"âŒ {package} - Missing\")\n",
    "        missing_packages.append(package)\n",
    "\n",
    "if missing_packages:\n",
    "    print(f\"\\nInstall missing packages with:\")\n",
    "    print(f\"pip install {' '.join(missing_packages)}\")\n",
    "else:\n",
    "    print(\"\\nðŸŽ‰ All required packages are available!\")\n",
    "\n",
    "# Check Sol-specific modules (these should be available when running on Sol)\n",
    "print(\"\\n--- Sol-specific checks ---\")\n",
    "sol_modules = ['subprocess', 'uuid', 'pathlib', 'json', 'tempfile']\n",
    "for module in sol_modules:\n",
    "    try:\n",
    "        __import__(module)\n",
    "        print(f\"âœ… {module} - Available\")\n",
    "    except ImportError:\n",
    "        print(f\"âŒ {module} - Missing (this should not happen)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591264d3",
   "metadata": {},
   "source": [
    "# GPU Mentor: Enhanced RAG with Code Execution & Benchmarking\n",
    "\n",
    "This enhanced version of the Agentic RAG system includes:\n",
    "- **Code Execution on Sol**: Submit and execute user code on Sol's GPU nodes\n",
    "- **Performance Benchmarking**: Compare CPU vs GPU performance with RAPIDS libraries\n",
    "- **Code Optimization**: Automatically suggest GPU-accelerated alternatives\n",
    "- **Interactive Learning**: Socratic questioning to guide learning\n",
    "\n",
    "## Architecture Overview\n",
    "1. **RAG Agent**: Existing system for answering questions about GPU acceleration\n",
    "2. **Code Executor**: Submits jobs to Sol via SLURM\n",
    "3. **Benchmark Engine**: Measures and compares CPU/GPU performance\n",
    "4. **Code Optimizer**: Suggests RAPIDS/CuPy alternatives\n",
    "5. **Enhanced UI**: Comprehensive interface for code playground and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40018daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced imports for GPU Mentor\n",
    "import subprocess\n",
    "import tempfile\n",
    "import time\n",
    "import json\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime\n",
    "import ast\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f6dca4",
   "metadata": {},
   "source": [
    "## 11. Sol Code Executor - SLURM Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b85430",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolCodeExecutor:\n",
    "    \"\"\"\n",
    "    Executes code on Sol supercomputer via SLURM job submission.\n",
    "    Handles both CPU and GPU benchmarking jobs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_work_dir=\"/tmp/gpu_mentor\"):\n",
    "        self.base_work_dir = Path(base_work_dir)\n",
    "        self.base_work_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "    def create_slurm_script(self, code: str, job_type: str = \"cpu\", \n",
    "                           time_limit: str = \"00:15:00\", \n",
    "                           memory: str = \"32G\") -> str:\n",
    "        \"\"\"Create SLURM batch script for code execution.\"\"\"\n",
    "        \n",
    "        job_id = str(uuid.uuid4())[:8]\n",
    "        script_content = \"\"\n",
    "        \n",
    "        if job_type == \"cpu\":\n",
    "            script_content = f\"\"\"#!/bin/bash\n",
    "#SBATCH --job-name=gpu_mentor_cpu_{job_id}\n",
    "#SBATCH --partition=general\n",
    "#SBATCH --qos=public\n",
    "#SBATCH --time={time_limit}\n",
    "#SBATCH --cpus-per-task=8\n",
    "#SBATCH --mem={memory}\n",
    "#SBATCH --output=cpu_output_{job_id}.out\n",
    "#SBATCH --error=cpu_error_{job_id}.err\n",
    "\n",
    "# Load necessary modules\n",
    "module load python/3.11\n",
    "module load anaconda3\n",
    "\n",
    "# Activate conda environment with CPU libraries\n",
    "source activate base\n",
    "\n",
    "# Create timing wrapper\n",
    "cat > benchmark_cpu_{job_id}.py << 'SCRIPT_EOF'\n",
    "import time\n",
    "import sys\n",
    "import traceback\n",
    "import json\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "try:\n",
    "{self._indent_code(code)}\n",
    "    execution_status = \"success\"\n",
    "    error_message = \"\"\n",
    "except Exception as e:\n",
    "    execution_status = \"error\"\n",
    "    error_message = str(e)\n",
    "    traceback.print_exc()\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Save benchmark results\n",
    "results = {{\n",
    "    \"execution_time\": execution_time,\n",
    "    \"job_type\": \"cpu\",\n",
    "    \"job_id\": \"{job_id}\",\n",
    "    \"status\": execution_status,\n",
    "    \"error\": error_message\n",
    "}}\n",
    "\n",
    "with open(\"cpu_benchmark_{job_id}.json\", \"w\") as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "print(f\"CPU Execution time: {{execution_time:.4f}} seconds\")\n",
    "SCRIPT_EOF\n",
    "\n",
    "# Execute the benchmark script\n",
    "python benchmark_cpu_{job_id}.py\n",
    "\"\"\"\n",
    "        else:  # GPU job\n",
    "            script_content = f\"\"\"#!/bin/bash\n",
    "#SBATCH --job-name=gpu_mentor_gpu_{job_id}\n",
    "#SBATCH --partition=general\n",
    "#SBATCH --qos=public\n",
    "#SBATCH --time={time_limit}\n",
    "#SBATCH --cpus-per-task=8\n",
    "#SBATCH --mem={memory}\n",
    "#SBATCH --gres=gpu:1\n",
    "#SBATCH --output=gpu_output_{job_id}.out\n",
    "#SBATCH --error=gpu_error_{job_id}.err\n",
    "\n",
    "# Load necessary modules\n",
    "module load python/3.11\n",
    "module load anaconda3\n",
    "module load cuda/12.1\n",
    "\n",
    "# Activate conda environment with GPU libraries\n",
    "source activate rapids-23.08\n",
    "\n",
    "# Create timing wrapper\n",
    "cat > benchmark_gpu_{job_id}.py << 'SCRIPT_EOF'\n",
    "import time\n",
    "import sys\n",
    "import traceback\n",
    "import json\n",
    "\n",
    "# Import GPU libraries\n",
    "try:\n",
    "    import cupy as cp\n",
    "    import cudf\n",
    "    import cuml\n",
    "    gpu_available = True\n",
    "except ImportError as e:\n",
    "    print(f\"GPU libraries not available: {{e}}\")\n",
    "    gpu_available = False\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "try:\n",
    "{self._indent_code(code)}\n",
    "    execution_status = \"success\"\n",
    "    error_message = \"\"\n",
    "except Exception as e:\n",
    "    execution_status = \"error\"\n",
    "    error_message = str(e)\n",
    "    traceback.print_exc()\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Save benchmark results\n",
    "results = {{\n",
    "    \"execution_time\": execution_time,\n",
    "    \"job_type\": \"gpu\",\n",
    "    \"job_id\": \"{job_id}\",\n",
    "    \"status\": execution_status,\n",
    "    \"error\": error_message,\n",
    "    \"gpu_available\": gpu_available\n",
    "}}\n",
    "\n",
    "with open(\"gpu_benchmark_{job_id}.json\", \"w\") as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "print(f\"GPU Execution time: {{execution_time:.4f}} seconds\")\n",
    "SCRIPT_EOF\n",
    "\n",
    "# Execute the benchmark script\n",
    "python benchmark_gpu_{job_id}.py\n",
    "\"\"\"\n",
    "        \n",
    "        return script_content, job_id\n",
    "    \n",
    "    def _indent_code(self, code: str, indent: str = \"    \") -> str:\n",
    "        \"\"\"Add proper indentation to user code for embedding in script.\"\"\"\n",
    "        return \"\\n\".join(indent + line for line in code.split(\"\\n\"))\n",
    "    \n",
    "    def submit_job(self, script_content: str, job_id: str) -> str:\n",
    "        \"\"\"Submit job to SLURM and return job ID.\"\"\"\n",
    "        script_path = self.base_work_dir / f\"job_{job_id}.sh\"\n",
    "        \n",
    "        with open(script_path, 'w') as f:\n",
    "            f.write(script_content)\n",
    "        \n",
    "        try:\n",
    "            # Submit job via sbatch\n",
    "            result = subprocess.run(\n",
    "                [\"sbatch\", str(script_path)],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                cwd=self.base_work_dir\n",
    "            )\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                # Extract SLURM job ID from output\n",
    "                slurm_job_id = result.stdout.strip().split()[-1]\n",
    "                return slurm_job_id\n",
    "            else:\n",
    "                raise Exception(f\"Job submission failed: {result.stderr}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error submitting job: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def check_job_status(self, slurm_job_id: str) -> str:\n",
    "        \"\"\"Check the status of a SLURM job.\"\"\"\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"squeue\", \"-j\", slurm_job_id, \"-h\", \"-o\", \"%T\"],\n",
    "                capture_output=True,\n",
    "                text=True\n",
    "            )\n",
    "            \n",
    "            if result.returncode == 0 and result.stdout.strip():\n",
    "                return result.stdout.strip()\n",
    "            else:\n",
    "                # Job might be completed, check sacct\n",
    "                result = subprocess.run(\n",
    "                    [\"sacct\", \"-j\", slurm_job_id, \"-n\", \"-o\", \"State\"],\n",
    "                    capture_output=True,\n",
    "                    text=True\n",
    "                )\n",
    "                if result.returncode == 0 and result.stdout.strip():\n",
    "                    return result.stdout.strip().split()[0]\n",
    "                else:\n",
    "                    return \"UNKNOWN\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking job status: {e}\")\n",
    "            return \"ERROR\"\n",
    "    \n",
    "    def get_job_results(self, job_id: str, job_type: str) -> Dict:\n",
    "        \"\"\"Retrieve benchmark results from completed job.\"\"\"\n",
    "        result_file = self.base_work_dir / f\"{job_type}_benchmark_{job_id}.json\"\n",
    "        \n",
    "        if result_file.exists():\n",
    "            with open(result_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "        else:\n",
    "            return {\"error\": \"Results file not found\"}\n",
    "    \n",
    "    def cleanup_job_files(self, job_id: str):\n",
    "        \"\"\"Clean up temporary job files.\"\"\"\n",
    "        patterns = [\n",
    "            f\"job_{job_id}.sh\",\n",
    "            f\"*_output_{job_id}.out\",\n",
    "            f\"*_error_{job_id}.err\",\n",
    "            f\"*_benchmark_{job_id}.py\",\n",
    "            f\"*_benchmark_{job_id}.json\"\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            for file_path in self.base_work_dir.glob(pattern):\n",
    "                try:\n",
    "                    file_path.unlink()\n",
    "                except Exception as e:\n",
    "                    print(f\"Error cleaning up {file_path}: {e}\")\n",
    "\n",
    "# Initialize the Sol executor\n",
    "sol_executor = SolCodeExecutor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e1440d",
   "metadata": {},
   "source": [
    "## 12. Code Optimizer - GPU Acceleration Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7c9584",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeOptimizer:\n",
    "    \"\"\"\n",
    "    Analyzes user code and suggests GPU-accelerated alternatives using RAPIDS and CuPy.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.optimization_patterns = {\n",
    "            # NumPy to CuPy optimizations\n",
    "            'numpy': {\n",
    "                'import numpy as np': 'import cupy as np',\n",
    "                'np.array(': 'cp.array(',\n",
    "                'np.random.': 'cp.random.',\n",
    "                'np.linalg.': 'cp.linalg.',\n",
    "                'np.fft.': 'cp.fft.',\n",
    "                '.cpu()': '',  # Remove .cpu() calls\n",
    "            },\n",
    "            \n",
    "            # Pandas to cuDF optimizations\n",
    "            'pandas': {\n",
    "                'import pandas as pd': 'import cudf as pd',\n",
    "                'pd.DataFrame(': 'cudf.DataFrame(',\n",
    "                'pd.Series(': 'cudf.Series(',\n",
    "                'pd.read_csv(': 'cudf.read_csv(',\n",
    "                'pd.read_parquet(': 'cudf.read_parquet(',\n",
    "                '.to_pandas()': '',  # Remove .to_pandas() calls\n",
    "            },\n",
    "            \n",
    "            # Scikit-learn to cuML optimizations\n",
    "            'sklearn': {\n",
    "                'from sklearn.': 'from cuml.',\n",
    "                'sklearn.': 'cuml.',\n",
    "            },\n",
    "            \n",
    "            # Dask optimizations\n",
    "            'dask': {\n",
    "                'import dask.array as da': 'import dask.array as da\\\\n# Configure Dask to use CuPy backend\\\\nimport dask\\\\ndask.config.set({\"array.backend\": \"cupy\"})',\n",
    "                'import dask.dataframe as dd': 'import dask_cudf as dd',\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def analyze_code(self, code: str) -> Dict[str, any]:\n",
    "        \"\"\"Analyze code for optimization opportunities.\"\"\"\n",
    "        analysis = {\n",
    "            'libraries_detected': [],\n",
    "            'optimization_opportunities': [],\n",
    "            'estimated_speedup': 1.0,\n",
    "            'gpu_compatible': True,\n",
    "            'warnings': []\n",
    "        }\n",
    "        \n",
    "        # Detect libraries used\n",
    "        for lib_type, patterns in self.optimization_patterns.items():\n",
    "            for pattern in patterns.keys():\n",
    "                if pattern in code:\n",
    "                    analysis['libraries_detected'].append(lib_type)\n",
    "                    break\n",
    "        \n",
    "        # Check for GPU incompatible operations\n",
    "        incompatible_patterns = [\n",
    "            'matplotlib.pyplot',  # Plotting might need CPU arrays\n",
    "            'pickle.dump',        # Serialization issues\n",
    "            'multiprocessing',    # GPU memory management conflicts\n",
    "        ]\n",
    "        \n",
    "        for pattern in incompatible_patterns:\n",
    "            if pattern in code:\n",
    "                analysis['warnings'].append(f\"Detected {pattern} - may require CPU data conversion\")\n",
    "        \n",
    "        # Estimate potential speedup based on operations\n",
    "        compute_intensive_ops = [\n",
    "            'np.dot', 'np.matmul', '@',  # Matrix operations\n",
    "            'np.fft', 'scipy.fft',       # FFT operations\n",
    "            '.groupby(', '.agg(',        # Aggregation operations\n",
    "            'for ' in code and 'range(' in code,  # Loops that could be vectorized\n",
    "        ]\n",
    "        \n",
    "        speedup_factors = []\n",
    "        for op in compute_intensive_ops:\n",
    "            if isinstance(op, bool):\n",
    "                if op:\n",
    "                    speedup_factors.append(5.0)  # Loop vectorization\n",
    "            elif op in code:\n",
    "                if 'matmul' in op or 'dot' in op or '@' in op:\n",
    "                    speedup_factors.append(10.0)  # Matrix ops\n",
    "                elif 'fft' in op:\n",
    "                    speedup_factors.append(15.0)  # FFT ops\n",
    "                else:\n",
    "                    speedup_factors.append(3.0)   # Other ops\n",
    "        \n",
    "        if speedup_factors:\n",
    "            analysis['estimated_speedup'] = max(speedup_factors)\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def suggest_optimizations(self, code: str) -> str:\n",
    "        \"\"\"Generate GPU-optimized version of the code.\"\"\"\n",
    "        optimized_code = code\n",
    "        \n",
    "        # Apply optimization patterns\n",
    "        for lib_type, patterns in self.optimization_patterns.items():\n",
    "            for old_pattern, new_pattern in patterns.items():\n",
    "                optimized_code = optimized_code.replace(old_pattern, new_pattern)\n",
    "        \n",
    "        # Add GPU-specific optimizations\n",
    "        if 'import cupy' in optimized_code and 'import cupy as np' not in optimized_code:\n",
    "            optimized_code = 'import cupy as cp\\\\n' + optimized_code\n",
    "        \n",
    "        # Add memory pool for better performance\n",
    "        if 'cupy' in optimized_code:\n",
    "            memory_pool_code = \"\"\"\n",
    "# Enable CuPy memory pool for better performance\n",
    "import cupy\n",
    "mempool = cupy.get_default_memory_pool()\n",
    "pinned_mempool = cupy.get_default_pinned_memory_pool()\n",
    "\"\"\"\n",
    "            optimized_code = memory_pool_code + optimized_code\n",
    "        \n",
    "        return optimized_code\n",
    "    \n",
    "    def create_benchmark_code(self, original_code: str, optimized_code: str) -> Tuple[str, str]:\n",
    "        \"\"\"Create side-by-side benchmark versions.\"\"\"\n",
    "        \n",
    "        cpu_benchmark = f\"\"\"\n",
    "# CPU Version Benchmark\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "{original_code}\n",
    "\"\"\"\n",
    "        \n",
    "        gpu_benchmark = f\"\"\"\n",
    "# GPU Version Benchmark  \n",
    "import time\n",
    "import cupy as cp\n",
    "import cudf as pd\n",
    "\n",
    "{optimized_code}\n",
    "\n",
    "# Convert final results back to CPU for comparison if needed\n",
    "# result = cp.asnumpy(result) if hasattr(result, 'get') else result\n",
    "\"\"\"\n",
    "        \n",
    "        return cpu_benchmark, gpu_benchmark\n",
    "\n",
    "# Initialize the code optimizer\n",
    "code_optimizer = CodeOptimizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22780b6",
   "metadata": {},
   "source": [
    "## 13. Benchmark Engine - Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bda287",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BenchmarkEngine:\n",
    "    \"\"\"\n",
    "    Coordinates CPU vs GPU benchmarking using Sol's compute resources.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sol_executor: SolCodeExecutor, code_optimizer: CodeOptimizer):\n",
    "        self.sol_executor = sol_executor\n",
    "        self.code_optimizer = code_optimizer\n",
    "        self.benchmark_history = []\n",
    "    \n",
    "    def run_comprehensive_benchmark(self, user_code: str, timeout: int = 300) -> Dict:\n",
    "        \"\"\"\n",
    "        Run comprehensive CPU vs GPU benchmark.\n",
    "        \n",
    "        Args:\n",
    "            user_code: Original user code to benchmark\n",
    "            timeout: Maximum wait time for jobs to complete (seconds)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with benchmark results and visualizations\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"ðŸ” Analyzing code for optimization opportunities...\")\n",
    "        analysis = self.code_optimizer.analyze_code(user_code)\n",
    "        \n",
    "        print(\"âš¡ Generating GPU-optimized version...\")\n",
    "        optimized_code = self.code_optimizer.suggest_optimizations(user_code)\n",
    "        \n",
    "        # Create benchmark versions\n",
    "        cpu_code, gpu_code = self.code_optimizer.create_benchmark_code(user_code, optimized_code)\n",
    "        \n",
    "        print(\"ðŸš€ Submitting jobs to Sol...\")\n",
    "        \n",
    "        # Submit CPU job\n",
    "        cpu_script, cpu_job_id = self.sol_executor.create_slurm_script(\n",
    "            cpu_code, job_type=\"cpu\", time_limit=\"00:15:00\"\n",
    "        )\n",
    "        cpu_slurm_id = self.sol_executor.submit_job(cpu_script, cpu_job_id)\n",
    "        \n",
    "        # Submit GPU job\n",
    "        gpu_script, gpu_job_id = self.sol_executor.create_slurm_script(\n",
    "            gpu_code, job_type=\"gpu\", time_limit=\"00:15:00\"\n",
    "        )\n",
    "        gpu_slurm_id = self.sol_executor.submit_job(gpu_script, gpu_job_id)\n",
    "        \n",
    "        if not cpu_slurm_id or not gpu_slurm_id:\n",
    "            return {\"error\": \"Failed to submit jobs to Sol\"}\n",
    "        \n",
    "        print(f\"âœ… Jobs submitted: CPU ({cpu_slurm_id}), GPU ({gpu_slurm_id})\")\n",
    "        print(\"â³ Waiting for jobs to complete...\")\n",
    "        \n",
    "        # Wait for jobs to complete\n",
    "        start_wait = time.time()\n",
    "        cpu_status = gpu_status = \"PENDING\"\n",
    "        \n",
    "        while time.time() - start_wait < timeout:\n",
    "            cpu_status = self.sol_executor.check_job_status(cpu_slurm_id)\n",
    "            gpu_status = self.sol_executor.check_job_status(gpu_slurm_id)\n",
    "            \n",
    "            print(f\"ðŸ“Š Status - CPU: {cpu_status}, GPU: {gpu_status}\")\n",
    "            \n",
    "            if cpu_status in [\"COMPLETED\", \"FAILED\"] and gpu_status in [\"COMPLETED\", \"FAILED\"]:\n",
    "                break\n",
    "                \n",
    "            time.sleep(10)  # Check every 10 seconds\n",
    "        \n",
    "        # Collect results\n",
    "        print(\"ðŸ“ˆ Collecting benchmark results...\")\n",
    "        cpu_results = self.sol_executor.get_job_results(cpu_job_id, \"cpu\")\n",
    "        gpu_results = self.sol_executor.get_job_results(gpu_job_id, \"gpu\")\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        benchmark_results = self._process_results(\n",
    "            cpu_results, gpu_results, analysis, user_code, optimized_code\n",
    "        )\n",
    "        \n",
    "        # Store in history\n",
    "        self.benchmark_history.append({\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"results\": benchmark_results\n",
    "        })\n",
    "        \n",
    "        # Cleanup\n",
    "        self.sol_executor.cleanup_job_files(cpu_job_id)\n",
    "        self.sol_executor.cleanup_job_files(gpu_job_id)\n",
    "        \n",
    "        return benchmark_results\n",
    "    \n",
    "    def _process_results(self, cpu_results: Dict, gpu_results: Dict, \n",
    "                        analysis: Dict, original_code: str, optimized_code: str) -> Dict:\n",
    "        \"\"\"Process and format benchmark results.\"\"\"\n",
    "        \n",
    "        results = {\n",
    "            \"analysis\": analysis,\n",
    "            \"original_code\": original_code,\n",
    "            \"optimized_code\": optimized_code,\n",
    "            \"cpu_results\": cpu_results,\n",
    "            \"gpu_results\": gpu_results,\n",
    "            \"performance_metrics\": {},\n",
    "            \"recommendations\": []\n",
    "        }\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        if (cpu_results.get(\"status\") == \"success\" and \n",
    "            gpu_results.get(\"status\") == \"success\"):\n",
    "            \n",
    "            cpu_time = cpu_results.get(\"execution_time\", 0)\n",
    "            gpu_time = gpu_results.get(\"execution_time\", 0)\n",
    "            \n",
    "            if cpu_time > 0 and gpu_time > 0:\n",
    "                speedup = cpu_time / gpu_time\n",
    "                efficiency = (speedup / analysis.get(\"estimated_speedup\", 1.0)) * 100\n",
    "                \n",
    "                results[\"performance_metrics\"] = {\n",
    "                    \"cpu_execution_time\": cpu_time,\n",
    "                    \"gpu_execution_time\": gpu_time,\n",
    "                    \"speedup_factor\": speedup,\n",
    "                    \"efficiency_percent\": efficiency,\n",
    "                    \"time_saved\": cpu_time - gpu_time,\n",
    "                    \"percent_improvement\": ((cpu_time - gpu_time) / cpu_time) * 100\n",
    "                }\n",
    "        \n",
    "        # Generate recommendations\n",
    "        results[\"recommendations\"] = self._generate_recommendations(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _generate_recommendations(self, results: Dict) -> List[str]:\n",
    "        \"\"\"Generate educational recommendations based on benchmark results.\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        metrics = results.get(\"performance_metrics\", {})\n",
    "        speedup = metrics.get(\"speedup_factor\", 1.0)\n",
    "        \n",
    "        if speedup > 5:\n",
    "            recommendations.append(\"ðŸŽ‰ Excellent GPU acceleration! This workload benefits significantly from parallel processing.\")\n",
    "        elif speedup > 2:\n",
    "            recommendations.append(\"âœ… Good GPU speedup achieved. Consider optimizing memory access patterns for even better performance.\")\n",
    "        elif speedup > 1.1:\n",
    "            recommendations.append(\"ðŸ“ˆ Modest improvement with GPU. This workload may be memory-bound or have limited parallelism.\")\n",
    "        else:\n",
    "            recommendations.append(\"âš ï¸ Limited GPU benefit. Consider if this workload has sufficient computational complexity.\")\n",
    "        \n",
    "        # Check for optimization opportunities\n",
    "        analysis = results.get(\"analysis\", {})\n",
    "        if \"numpy\" in analysis.get(\"libraries_detected\", []):\n",
    "            recommendations.append(\"ðŸ’¡ Consider using CuPy's memory pool for better performance with repeated operations.\")\n",
    "        \n",
    "        if \"pandas\" in analysis.get(\"libraries_detected\", []):\n",
    "            recommendations.append(\"ðŸ“Š cuDF provides GPU-accelerated dataframe operations similar to pandas.\")\n",
    "        \n",
    "        if analysis.get(\"warnings\"):\n",
    "            recommendations.append(\"âš ï¸ Some operations may require CPU-GPU memory transfers. Profile memory usage.\")\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def create_visualization(self, benchmark_results: Dict) -> go.Figure:\n",
    "        \"\"\"Create interactive visualization of benchmark results.\"\"\"\n",
    "        \n",
    "        metrics = benchmark_results.get(\"performance_metrics\", {})\n",
    "        \n",
    "        if not metrics:\n",
    "            # Create error visualization\n",
    "            fig = go.Figure()\n",
    "            fig.add_annotation(\n",
    "                text=\"Benchmark data not available\",\n",
    "                xref=\"paper\", yref=\"paper\",\n",
    "                x=0.5, y=0.5, showarrow=False,\n",
    "                font=dict(size=20)\n",
    "            )\n",
    "            return fig\n",
    "        \n",
    "        # Create comparison chart\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # Execution time comparison\n",
    "        fig.add_trace(go.Bar(\n",
    "            name='CPU',\n",
    "            x=['Execution Time'],\n",
    "            y=[metrics[\"cpu_execution_time\"]],\n",
    "            marker_color='lightcoral',\n",
    "            text=[f\"{metrics['cpu_execution_time']:.3f}s\"],\n",
    "            textposition='auto'\n",
    "        ))\n",
    "        \n",
    "        fig.add_trace(go.Bar(\n",
    "            name='GPU',\n",
    "            x=['Execution Time'],\n",
    "            y=[metrics[\"gpu_execution_time\"]],\n",
    "            marker_color='lightblue',\n",
    "            text=[f\"{metrics['gpu_execution_time']:.3f}s\"],\n",
    "            textposition='auto'\n",
    "        ))\n",
    "        \n",
    "        # Add speedup annotation\n",
    "        speedup = metrics.get(\"speedup_factor\", 1.0)\n",
    "        fig.add_annotation(\n",
    "            text=f\"ðŸš€ {speedup:.1f}x Speedup\",\n",
    "            xref=\"paper\", yref=\"paper\",\n",
    "            x=0.7, y=0.9,\n",
    "            showarrow=False,\n",
    "            font=dict(size=16, color=\"green\"),\n",
    "            bgcolor=\"lightyellow\",\n",
    "            bordercolor=\"orange\",\n",
    "            borderwidth=2\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=\"CPU vs GPU Performance Comparison\",\n",
    "            yaxis_title=\"Execution Time (seconds)\",\n",
    "            barmode='group',\n",
    "            template=\"plotly_white\"\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "\n",
    "# Initialize the benchmark engine\n",
    "benchmark_engine = BenchmarkEngine(sol_executor, code_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a15023",
   "metadata": {},
   "source": [
    "## 14. Enhanced GPU Mentor Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06beda19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedGPUMentor:\n",
    "    \"\"\"\n",
    "    Enhanced GPU Mentor that combines RAG capabilities with code execution and analysis.\n",
    "    Integrates code input directly with LLM for comprehensive responses.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rag_graph, benchmark_engine: BenchmarkEngine, code_optimizer: CodeOptimizer):\n",
    "        self.rag_graph = rag_graph\n",
    "        self.benchmark_engine = benchmark_engine\n",
    "        self.code_optimizer = code_optimizer\n",
    "        self.conversation_history = []\n",
    "        self.code_execution_results = []\n",
    "    \n",
    "    def process_user_input(self, user_input: str, code: str = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Process user input with optional code, feeding both to LLM for integrated response.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = {\n",
    "            \"text_response\": \"\",\n",
    "            \"code_analysis\": None,\n",
    "            \"code_output\": None,\n",
    "            \"optimized_code\": None,\n",
    "            \"socratic_questions\": [],\n",
    "            \"learning_objectives\": []\n",
    "        }\n",
    "        \n",
    "        # Create enhanced prompt that includes code context if provided\n",
    "        enhanced_prompt = self._create_enhanced_prompt(user_input, code)\n",
    "        \n",
    "        # Get RAG response with code context\n",
    "        rag_result = self.rag_graph.invoke({\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": enhanced_prompt}]\n",
    "        })\n",
    "        response[\"text_response\"] = rag_result[\"messages\"][-1].content\n",
    "        \n",
    "        # If code is provided, analyze and execute it\n",
    "        if code and code.strip():\n",
    "            print(\"ðŸ” Analyzing provided code...\")\n",
    "            \n",
    "            # Analyze code for optimization opportunities\n",
    "            analysis = self.code_optimizer.analyze_code(code)\n",
    "            response[\"code_analysis\"] = analysis\n",
    "            \n",
    "            # Generate optimized version\n",
    "            optimized_code = self.code_optimizer.suggest_optimizations(code)\n",
    "            response[\"optimized_code\"] = optimized_code\n",
    "            \n",
    "            # Execute code and capture output\n",
    "            print(\"âš¡ Executing code...\")\n",
    "            try:\n",
    "                code_output = self._execute_code_safely(code)\n",
    "                response[\"code_output\"] = code_output\n",
    "                \n",
    "                # Store execution results\n",
    "                self.code_execution_results.append({\n",
    "                    \"timestamp\": datetime.now().isoformat(),\n",
    "                    \"code\": code,\n",
    "                    \"output\": code_output,\n",
    "                    \"analysis\": analysis\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                response[\"code_output\"] = {\"error\": f\"Code execution failed: {str(e)}\"}\n",
    "            \n",
    "            # Generate educational content based on code and context\n",
    "            response[\"socratic_questions\"] = self._generate_socratic_questions(analysis, user_input, code)\n",
    "            response[\"learning_objectives\"] = self._generate_learning_objectives(analysis, code)\n",
    "        \n",
    "        # Store conversation\n",
    "        self.conversation_history.append({\n",
    "            \"user_input\": user_input,\n",
    "            \"code\": code,\n",
    "            \"response\": response,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def _create_enhanced_prompt(self, user_input: str, code: str = None) -> str:\n",
    "        \"\"\"Create enhanced prompt that includes code context for the LLM.\"\"\"\n",
    "        \n",
    "        if not code or not code.strip():\n",
    "            return user_input\n",
    "        \n",
    "        enhanced_prompt = f\"\"\"\n",
    "User Question: {user_input}\n",
    "\n",
    "User's Python Code:\n",
    "```python\n",
    "{code}\n",
    "```\n",
    "\n",
    "Please analyze this code in the context of the user's question. Consider:\n",
    "1. How the code relates to the question being asked\n",
    "2. Potential GPU acceleration opportunities in this specific code\n",
    "3. Any issues, optimizations, or improvements you can suggest\n",
    "4. Educational insights about GPU acceleration concepts demonstrated in this code\n",
    "\n",
    "Provide a comprehensive response that addresses both the question and the code together.\n",
    "\"\"\"\n",
    "        return enhanced_prompt\n",
    "    \n",
    "    def _execute_code_safely(self, code: str) -> Dict:\n",
    "        \"\"\"Execute code safely and capture output.\"\"\"\n",
    "        \n",
    "        import io\n",
    "        import sys\n",
    "        import contextlib\n",
    "        \n",
    "        # Capture stdout and stderr\n",
    "        old_stdout = sys.stdout\n",
    "        old_stderr = sys.stderr\n",
    "        stdout_capture = io.StringIO()\n",
    "        stderr_capture = io.StringIO()\n",
    "        \n",
    "        execution_result = {\n",
    "            \"stdout\": \"\",\n",
    "            \"stderr\": \"\",\n",
    "            \"variables\": {},\n",
    "            \"execution_time\": 0,\n",
    "            \"status\": \"success\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            start_time = time.perf_counter()\n",
    "            \n",
    "            # Redirect output\n",
    "            sys.stdout = stdout_capture\n",
    "            sys.stderr = stderr_capture\n",
    "            \n",
    "            # Create a safe execution environment\n",
    "            safe_globals = {\n",
    "                '__builtins__': __builtins__,\n",
    "                'print': print,\n",
    "                'len': len,\n",
    "                'range': range,\n",
    "                'enumerate': enumerate,\n",
    "                'zip': zip,\n",
    "                'sum': sum,\n",
    "                'max': max,\n",
    "                'min': min,\n",
    "                'abs': abs,\n",
    "                'round': round,\n",
    "                'type': type,\n",
    "                'str': str,\n",
    "                'int': int,\n",
    "                'float': float,\n",
    "                'list': list,\n",
    "                'dict': dict,\n",
    "                'tuple': tuple,\n",
    "                'set': set,\n",
    "            }\n",
    "            \n",
    "            # Add commonly used libraries\n",
    "            try:\n",
    "                import numpy as np\n",
    "                safe_globals['np'] = np\n",
    "                safe_globals['numpy'] = np\n",
    "            except ImportError:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                import pandas as pd\n",
    "                safe_globals['pd'] = pd\n",
    "                safe_globals['pandas'] = pd\n",
    "            except ImportError:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                import matplotlib.pyplot as plt\n",
    "                safe_globals['plt'] = plt\n",
    "            except ImportError:\n",
    "                pass\n",
    "            \n",
    "            local_vars = {}\n",
    "            \n",
    "            # Execute the code\n",
    "            exec(code, safe_globals, local_vars)\n",
    "            \n",
    "            end_time = time.perf_counter()\n",
    "            execution_result[\"execution_time\"] = end_time - start_time\n",
    "            \n",
    "            # Capture variables (limit to avoid memory issues)\n",
    "            for name, value in local_vars.items():\n",
    "                if not name.startswith('_'):\n",
    "                    try:\n",
    "                        # Only store basic info about complex objects\n",
    "                        if hasattr(value, 'shape'):  # numpy arrays, pandas objects\n",
    "                            execution_result[\"variables\"][name] = f\"{type(value).__name__} with shape {value.shape}\"\n",
    "                        elif hasattr(value, '__len__') and len(value) > 100:\n",
    "                            execution_result[\"variables\"][name] = f\"{type(value).__name__} with {len(value)} elements\"\n",
    "                        elif isinstance(value, (int, float, str, bool, list, dict, tuple)) and len(str(value)) < 1000:\n",
    "                            execution_result[\"variables\"][name] = str(value)\n",
    "                        else:\n",
    "                            execution_result[\"variables\"][name] = f\"{type(value).__name__} object\"\n",
    "                    except:\n",
    "                        execution_result[\"variables\"][name] = f\"{type(value).__name__} object\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            execution_result[\"status\"] = \"error\"\n",
    "            execution_result[\"error\"] = str(e)\n",
    "            end_time = time.perf_counter()\n",
    "            execution_result[\"execution_time\"] = end_time - start_time\n",
    "        \n",
    "        finally:\n",
    "            # Restore stdout/stderr\n",
    "            sys.stdout = old_stdout\n",
    "            sys.stderr = old_stderr\n",
    "            \n",
    "            # Capture output\n",
    "            execution_result[\"stdout\"] = stdout_capture.getvalue()\n",
    "            execution_result[\"stderr\"] = stderr_capture.getvalue()\n",
    "        \n",
    "        return execution_result\n",
    "    \n",
    "    def _generate_socratic_questions(self, analysis: Dict, user_context: str, code: str) -> List[str]:\n",
    "        \"\"\"Generate Socratic questions based on code analysis and user context.\"\"\"\n",
    "        questions = []\n",
    "        \n",
    "        libraries = analysis.get(\"libraries_detected\", [])\n",
    "        estimated_speedup = analysis.get(\"estimated_speedup\", 1.0)\n",
    "        \n",
    "        # Code-specific questions\n",
    "        if \"numpy\" in libraries:\n",
    "            questions.extend([\n",
    "                \"Looking at your NumPy operations, which ones do you think would benefit most from GPU acceleration?\",\n",
    "                \"How might the memory access patterns in your code affect GPU performance?\",\n",
    "                \"What would happen to performance if you increased the array sizes by 10x?\"\n",
    "            ])\n",
    "        \n",
    "        if \"pandas\" in libraries:\n",
    "            questions.extend([\n",
    "                \"Which pandas operations in your code are most computationally expensive?\",\n",
    "                \"How would you modify this code to work with cuDF instead of pandas?\",\n",
    "                \"What considerations should you make when transferring data between CPU and GPU?\"\n",
    "            ])\n",
    "        \n",
    "        # Context-aware questions\n",
    "        if \"for \" in code and \"range(\" in code:\n",
    "            questions.append(\"Could you vectorize any of these loops to improve performance?\")\n",
    "        \n",
    "        if \"def \" in code:\n",
    "            questions.append(\"How could you modify this function to accept both CPU and GPU arrays?\")\n",
    "        \n",
    "        if estimated_speedup > 5:\n",
    "            questions.append(\"Your code has high parallelization potential. What makes it suitable for GPU acceleration?\")\n",
    "        elif estimated_speedup < 2:\n",
    "            questions.append(\"This code may not benefit much from GPU acceleration. Can you identify why?\")\n",
    "        \n",
    "        return questions[:3]  # Limit to avoid overwhelming\n",
    "    \n",
    "    def _generate_learning_objectives(self, analysis: Dict, code: str) -> List[str]:\n",
    "        \"\"\"Generate specific learning objectives based on the code and analysis.\"\"\"\n",
    "        objectives = []\n",
    "        \n",
    "        libraries = analysis.get(\"libraries_detected\", [])\n",
    "        \n",
    "        if \"numpy\" in libraries:\n",
    "            objectives.extend([\n",
    "                \"Understand when to use CuPy vs NumPy for your specific operations\",\n",
    "                \"Learn about GPU memory management for array operations\",\n",
    "                \"Master efficient data transfer between CPU and GPU\"\n",
    "            ])\n",
    "        \n",
    "        if \"pandas\" in libraries:\n",
    "            objectives.extend([\n",
    "                \"Compare cuDF vs pandas for your data processing workflow\",\n",
    "                \"Understand GPU memory requirements for dataframe operations\",\n",
    "                \"Learn efficient groupby and aggregation patterns on GPU\"\n",
    "            ])\n",
    "        \n",
    "        # Code-specific objectives\n",
    "        if \"for \" in code:\n",
    "            objectives.append(\"Explore vectorization techniques to eliminate loops\")\n",
    "        \n",
    "        if \"def \" in code:\n",
    "            objectives.append(\"Design functions that work efficiently with both CPU and GPU data\")\n",
    "        \n",
    "        return objectives\n",
    "    \n",
    "    def generate_tutorial_content(self, topic: str) -> str:\n",
    "        \"\"\"Generate comprehensive tutorial content on specific GPU acceleration topics.\"\"\"\n",
    "        \n",
    "        tutorial_prompt = f\"\"\"\n",
    "        Create a comprehensive tutorial on {topic} for GPU acceleration. Include:\n",
    "        1. Conceptual explanation\n",
    "        2. Code examples comparing CPU vs GPU approaches\n",
    "        3. Performance considerations\n",
    "        4. Best practices\n",
    "        5. Common pitfalls to avoid\n",
    "        \n",
    "        Focus on practical, hands-on learning with RAPIDS and CuPy libraries.\n",
    "        \"\"\"\n",
    "        \n",
    "        result = self.rag_graph.invoke({\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": tutorial_prompt}]\n",
    "        })\n",
    "        \n",
    "        return result[\"messages\"][-1].content\n",
    "    \n",
    "    def get_execution_summary(self) -> Dict:\n",
    "        \"\"\"Get summary of all code execution results.\"\"\"\n",
    "        if not self.code_execution_results:\n",
    "            return {\"message\": \"No code executed yet\"}\n",
    "        \n",
    "        summary = {\n",
    "            \"total_executions\": len(self.code_execution_results),\n",
    "            \"successful_executions\": 0,\n",
    "            \"failed_executions\": 0,\n",
    "            \"average_execution_time\": 0,\n",
    "            \"common_libraries\": [],\n",
    "            \"recent_outputs\": []\n",
    "        }\n",
    "        \n",
    "        execution_times = []\n",
    "        libraries_count = {}\n",
    "        \n",
    "        for result in self.code_execution_results[-10:]:  # Last 10 executions\n",
    "            if result.get(\"output\", {}).get(\"status\") == \"success\":\n",
    "                summary[\"successful_executions\"] += 1\n",
    "                exec_time = result.get(\"output\", {}).get(\"execution_time\", 0)\n",
    "                execution_times.append(exec_time)\n",
    "            else:\n",
    "                summary[\"failed_executions\"] += 1\n",
    "            \n",
    "            # Count libraries\n",
    "            for lib in result.get(\"analysis\", {}).get(\"libraries_detected\", []):\n",
    "                libraries_count[lib] = libraries_count.get(lib, 0) + 1\n",
    "            \n",
    "            # Add recent output summary\n",
    "            output = result.get(\"output\", {})\n",
    "            summary[\"recent_outputs\"].append({\n",
    "                \"timestamp\": result.get(\"timestamp\"),\n",
    "                \"status\": output.get(\"status\", \"unknown\"),\n",
    "                \"execution_time\": output.get(\"execution_time\", 0),\n",
    "                \"output_length\": len(output.get(\"stdout\", \"\"))\n",
    "            })\n",
    "        \n",
    "        if execution_times:\n",
    "            summary[\"average_execution_time\"] = sum(execution_times) / len(execution_times)\n",
    "        \n",
    "        summary[\"common_libraries\"] = sorted(libraries_count.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Initialize the enhanced GPU mentor\n",
    "gpu_mentor = EnhancedGPUMentor(graph, benchmark_engine, code_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525e3557",
   "metadata": {},
   "source": [
    "## 15. Enhanced Gradio Interface - GPU Mentor Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e20e06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BenchmarkEngine:\n",
    "    \"\"\"Comprehensive benchmarking engine for CPU vs GPU performance comparison.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.benchmark_results = []\n",
    "        self.predefined_benchmarks = self._setup_predefined_benchmarks()\n",
    "    \n",
    "    def _setup_predefined_benchmarks(self):\n",
    "        \"\"\"Setup predefined benchmarks based on NVIDIA Rapids techniques.\"\"\"\n",
    "        return {\n",
    "            \"Matrix Operations\": {\n",
    "                \"description\": \"Compare NumPy vs CuPy for large matrix operations\",\n",
    "                \"categories\": [\"Linear Algebra\", \"Array Processing\"],\n",
    "                \"benchmarks\": [\n",
    "                    {\n",
    "                        \"name\": \"Matrix Multiplication\",\n",
    "                        \"cpu_code\": \"\"\"\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Setup\n",
    "size = {size}\n",
    "A = np.random.rand(size, size).astype(np.float32)\n",
    "B = np.random.rand(size, size).astype(np.float32)\n",
    "\n",
    "# Benchmark\n",
    "start_time = time.perf_counter()\n",
    "C = np.matmul(A, B)\n",
    "cpu_time = time.perf_counter() - start_time\n",
    "\n",
    "result = {{\"execution_time\": cpu_time, \"result_shape\": C.shape, \"result_sum\": float(np.sum(C))}}\n",
    "\"\"\",\n",
    "                        \"gpu_code\": \"\"\"\n",
    "import cupy as cp\n",
    "import time\n",
    "\n",
    "# Setup\n",
    "size = {size}\n",
    "A = cp.random.rand(size, size).astype(cp.float32)\n",
    "B = cp.random.rand(size, size).astype(cp.float32)\n",
    "\n",
    "# Benchmark\n",
    "start_time = time.perf_counter()\n",
    "C = cp.matmul(A, B)\n",
    "cp.cuda.Device().synchronize()\n",
    "gpu_time = time.perf_counter() - start_time\n",
    "\n",
    "result = {{\"execution_time\": gpu_time, \"result_shape\": C.shape, \"result_sum\": float(cp.sum(C))}}\n",
    "\"\"\",\n",
    "                        \"sizes\": [256, 512, 1024, 2048],\n",
    "                        \"metric\": \"execution_time\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"Singular Value Decomposition\",\n",
    "                        \"cpu_code\": \"\"\"\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Setup\n",
    "size = {size}\n",
    "A = np.random.rand(size, size).astype(np.float32)\n",
    "\n",
    "# Benchmark\n",
    "start_time = time.perf_counter()\n",
    "U, s, Vt = np.linalg.svd(A)\n",
    "cpu_time = time.perf_counter() - start_time\n",
    "\n",
    "result = {{\"execution_time\": cpu_time, \"singular_values\": len(s), \"min_sv\": float(np.min(s))}}\n",
    "\"\"\",\n",
    "                        \"gpu_code\": \"\"\"\n",
    "import cupy as cp\n",
    "import time\n",
    "\n",
    "# Setup\n",
    "size = {size}\n",
    "A = cp.random.rand(size, size).astype(cp.float32)\n",
    "\n",
    "# Benchmark\n",
    "start_time = time.perf_counter()\n",
    "U, s, Vt = cp.linalg.svd(A)\n",
    "cp.cuda.Device().synchronize()\n",
    "gpu_time = time.perf_counter() - start_time\n",
    "\n",
    "result = {{\"execution_time\": gpu_time, \"singular_values\": len(s), \"min_sv\": float(cp.min(s))}}\n",
    "\"\"\",\n",
    "                        \"sizes\": [128, 256, 512, 1024],\n",
    "                        \"metric\": \"execution_time\"\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            \n",
    "            \"DataFrame Operations\": {\n",
    "                \"description\": \"Compare Pandas vs cuDF for data processing tasks\",\n",
    "                \"categories\": [\"Data Processing\", \"Analytics\"],\n",
    "                \"benchmarks\": [\n",
    "                    {\n",
    "                        \"name\": \"GroupBy Aggregation\",\n",
    "                        \"cpu_code\": \"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Setup\n",
    "n = {size}\n",
    "df = pd.DataFrame({{\n",
    "    'group': np.random.choice(['A', 'B', 'C', 'D', 'E'], n),\n",
    "    'value1': np.random.randn(n),\n",
    "    'value2': np.random.randn(n),\n",
    "    'value3': np.random.randint(1, 100, n)\n",
    "}})\n",
    "\n",
    "# Benchmark\n",
    "start_time = time.perf_counter()\n",
    "result_df = df.groupby('group').agg({{\n",
    "    'value1': ['mean', 'std', 'min', 'max'],\n",
    "    'value2': ['sum', 'count'],\n",
    "    'value3': ['median']\n",
    "}})\n",
    "cpu_time = time.perf_counter() - start_time\n",
    "\n",
    "result = {{\"execution_time\": cpu_time, \"groups\": len(result_df), \"total_rows\": len(df)}}\n",
    "\"\"\",\n",
    "                        \"gpu_code\": \"\"\"\n",
    "import cudf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Setup\n",
    "n = {size}\n",
    "df = cudf.DataFrame({{\n",
    "    'group': np.random.choice(['A', 'B', 'C', 'D', 'E'], n),\n",
    "    'value1': np.random.randn(n),\n",
    "    'value2': np.random.randn(n),\n",
    "    'value3': np.random.randint(1, 100, n)\n",
    "}})\n",
    "\n",
    "# Benchmark\n",
    "start_time = time.perf_counter()\n",
    "result_df = df.groupby('group').agg({{\n",
    "    'value1': ['mean', 'std', 'min', 'max'],\n",
    "    'value2': ['sum', 'count'],\n",
    "    'value3': ['mean']  # cuDF doesn't support median in groupby\n",
    "}})\n",
    "gpu_time = time.perf_counter() - start_time\n",
    "\n",
    "result = {{\"execution_time\": gpu_time, \"groups\": len(result_df), \"total_rows\": len(df)}}\n",
    "\"\"\",\n",
    "                        \"sizes\": [100000, 500000, 1000000, 2000000],\n",
    "                        \"metric\": \"execution_time\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"String Operations\",\n",
    "                        \"cpu_code\": \"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Setup\n",
    "n = {size}\n",
    "df = pd.DataFrame({{\n",
    "    'text': ['sample_text_' + str(i) for i in range(n)],\n",
    "    'category': np.random.choice(['cat', 'dog', 'bird'], n)\n",
    "}})\n",
    "\n",
    "# Benchmark\n",
    "start_time = time.perf_counter()\n",
    "df['text_upper'] = df['text'].str.upper()\n",
    "df['text_length'] = df['text'].str.len()\n",
    "df['contains_sample'] = df['text'].str.contains('sample')\n",
    "cpu_time = time.perf_counter() - start_time\n",
    "\n",
    "result = {{\"execution_time\": cpu_time, \"processed_strings\": len(df), \"avg_length\": df['text_length'].mean()}}\n",
    "\"\"\",\n",
    "                        \"gpu_code\": \"\"\"\n",
    "import cudf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Setup\n",
    "n = {size}\n",
    "df = cudf.DataFrame({{\n",
    "    'text': ['sample_text_' + str(i) for i in range(n)],\n",
    "    'category': np.random.choice(['cat', 'dog', 'bird'], n)\n",
    "}})\n",
    "\n",
    "# Benchmark\n",
    "start_time = time.perf_counter()\n",
    "df['text_upper'] = df['text'].str.upper()\n",
    "df['text_length'] = df['text'].str.len()\n",
    "df['contains_sample'] = df['text'].str.contains('sample')\n",
    "gpu_time = time.perf_counter() - start_time\n",
    "\n",
    "result = {{\"execution_time\": gpu_time, \"processed_strings\": len(df), \"avg_length\": df['text_length'].mean()}}\n",
    "\"\"\",\n",
    "                        \"sizes\": [50000, 100000, 250000, 500000],\n",
    "                        \"metric\": \"execution_time\"\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            \n",
    "            \"Machine Learning\": {\n",
    "                \"description\": \"Compare scikit-learn vs cuML for ML algorithms\",\n",
    "                \"categories\": [\"Machine Learning\", \"Classification\"],\n",
    "                \"benchmarks\": [\n",
    "                    {\n",
    "                        \"name\": \"K-Means Clustering\",\n",
    "                        \"cpu_code\": \"\"\"\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "# Setup\n",
    "n_samples = {size}\n",
    "n_features = 20\n",
    "X = np.random.rand(n_samples, n_features).astype(np.float32)\n",
    "\n",
    "# Benchmark\n",
    "start_time = time.perf_counter()\n",
    "kmeans = KMeans(n_clusters=8, random_state=42, n_init=10)\n",
    "labels = kmeans.fit_predict(X)\n",
    "cpu_time = time.perf_counter() - start_time\n",
    "\n",
    "result = {{\"execution_time\": cpu_time, \"n_clusters\": 8, \"inertia\": float(kmeans.inertia_)}}\n",
    "\"\"\",\n",
    "                        \"gpu_code\": \"\"\"\n",
    "import cupy as cp\n",
    "from cuml.cluster import KMeans\n",
    "import time\n",
    "\n",
    "# Setup\n",
    "n_samples = {size}\n",
    "n_features = 20\n",
    "X = cp.random.rand(n_samples, n_features).astype(cp.float32)\n",
    "\n",
    "# Benchmark\n",
    "start_time = time.perf_counter()\n",
    "kmeans = KMeans(n_clusters=8, random_state=42, n_init=10)\n",
    "labels = kmeans.fit_predict(X)\n",
    "cp.cuda.Device().synchronize()\n",
    "gpu_time = time.perf_counter() - start_time\n",
    "\n",
    "result = {{\"execution_time\": gpu_time, \"n_clusters\": 8, \"inertia\": float(kmeans.inertia_)}}\n",
    "\"\"\",\n",
    "                        \"sizes\": [10000, 50000, 100000, 200000],\n",
    "                        \"metric\": \"execution_time\"\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            \n",
    "            \"Mathematical Functions\": {\n",
    "                \"description\": \"Compare NumPy vs CuPy for mathematical operations\",\n",
    "                \"categories\": [\"Mathematics\", \"Signal Processing\"],\n",
    "                \"benchmarks\": [\n",
    "                    {\n",
    "                        \"name\": \"FFT Computation\",\n",
    "                        \"cpu_code\": \"\"\"\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Setup\n",
    "n = {size}\n",
    "x = np.random.randn(n).astype(np.complex64)\n",
    "\n",
    "# Benchmark\n",
    "start_time = time.perf_counter()\n",
    "fft_result = np.fft.fft(x)\n",
    "cpu_time = time.perf_counter() - start_time\n",
    "\n",
    "result = {{\"execution_time\": cpu_time, \"fft_size\": len(fft_result), \"max_magnitude\": float(np.max(np.abs(fft_result)))}}\n",
    "\"\"\",\n",
    "                        \"gpu_code\": \"\"\"\n",
    "import cupy as cp\n",
    "import time\n",
    "\n",
    "# Setup\n",
    "n = {size}\n",
    "x = cp.random.randn(n).astype(cp.complex64)\n",
    "\n",
    "# Benchmark\n",
    "start_time = time.perf_counter()\n",
    "fft_result = cp.fft.fft(x)\n",
    "cp.cuda.Device().synchronize()\n",
    "gpu_time = time.perf_counter() - start_time\n",
    "\n",
    "result = {{\"execution_time\": gpu_time, \"fft_size\": len(fft_result), \"max_magnitude\": float(cp.max(cp.abs(fft_result)))}}\n",
    "\"\"\",\n",
    "                        \"sizes\": [8192, 32768, 131072, 524288],\n",
    "                        \"metric\": \"execution_time\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def run_benchmark(self, category, benchmark_name, size):\n",
    "        \"\"\"Run a specific benchmark and return results.\"\"\"\n",
    "        if category not in self.predefined_benchmarks:\n",
    "            return None\n",
    "        \n",
    "        benchmark_data = None\n",
    "        for bench in self.predefined_benchmarks[category][\"benchmarks\"]:\n",
    "            if bench[\"name\"] == benchmark_name:\n",
    "                benchmark_data = bench\n",
    "                break\n",
    "        \n",
    "        if not benchmark_data:\n",
    "            return None\n",
    "        \n",
    "        # Prepare code with size parameter\n",
    "        cpu_code = benchmark_data[\"cpu_code\"].format(size=size)\n",
    "        gpu_code = benchmark_data[\"gpu_code\"].format(size=size)\n",
    "        \n",
    "        results = {\n",
    "            \"benchmark\": benchmark_name,\n",
    "            \"category\": category,\n",
    "            \"size\": size,\n",
    "            \"cpu_result\": None,\n",
    "            \"gpu_result\": None,\n",
    "            \"speedup\": None,\n",
    "            \"winner\": None,\n",
    "            \"error\": None\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Execute CPU code\n",
    "            cpu_globals = {}\n",
    "            exec(cpu_code, cpu_globals)\n",
    "            results[\"cpu_result\"] = cpu_globals.get(\"result\", {})\n",
    "            \n",
    "            # Execute GPU code (with error handling for missing GPU libraries)\n",
    "            try:\n",
    "                gpu_globals = {}\n",
    "                exec(gpu_code, gpu_globals)\n",
    "                results[\"gpu_result\"] = gpu_globals.get(\"result\", {})\n",
    "                \n",
    "                # Calculate speedup\n",
    "                if (results[\"cpu_result\"] and results[\"gpu_result\"] and \n",
    "                    \"execution_time\" in results[\"cpu_result\"] and \n",
    "                    \"execution_time\" in results[\"gpu_result\"]):\n",
    "                    \n",
    "                    cpu_time = results[\"cpu_result\"][\"execution_time\"]\n",
    "                    gpu_time = results[\"gpu_result\"][\"execution_time\"]\n",
    "                    \n",
    "                    if gpu_time > 0:\n",
    "                        results[\"speedup\"] = cpu_time / gpu_time\n",
    "                        results[\"winner\"] = \"GPU\" if results[\"speedup\"] > 1 else \"CPU\"\n",
    "                    \n",
    "            except ImportError as e:\n",
    "                results[\"error\"] = f\"GPU libraries not available: {str(e)}\"\n",
    "                results[\"gpu_result\"] = {\"error\": \"GPU libraries not available\"}\n",
    "            except Exception as e:\n",
    "                results[\"error\"] = f\"GPU execution failed: {str(e)}\"\n",
    "                results[\"gpu_result\"] = {\"error\": str(e)}\n",
    "                \n",
    "        except Exception as e:\n",
    "            results[\"error\"] = f\"Benchmark execution failed: {str(e)}\"\n",
    "        \n",
    "        # Store results\n",
    "        self.benchmark_results.append(results)\n",
    "        return results\n",
    "    \n",
    "    def get_benchmark_categories(self):\n",
    "        \"\"\"Get list of available benchmark categories.\"\"\"\n",
    "        return list(self.predefined_benchmarks.keys())\n",
    "    \n",
    "    def get_benchmarks_for_category(self, category):\n",
    "        \"\"\"Get list of benchmarks for a specific category.\"\"\"\n",
    "        if category in self.predefined_benchmarks:\n",
    "            return [bench[\"name\"] for bench in self.predefined_benchmarks[category][\"benchmarks\"]]\n",
    "        return []\n",
    "    \n",
    "    def get_benchmark_sizes(self, category, benchmark_name):\n",
    "        \"\"\"Get available sizes for a specific benchmark.\"\"\"\n",
    "        if category in self.predefined_benchmarks:\n",
    "            for bench in self.predefined_benchmarks[category][\"benchmarks\"]:\n",
    "                if bench[\"name\"] == benchmark_name:\n",
    "                    return bench[\"sizes\"]\n",
    "        return []\n",
    "    \n",
    "    def format_benchmark_results(self, results):\n",
    "        \"\"\"Format benchmark results for display.\"\"\"\n",
    "        if not results or results.get(\"error\"):\n",
    "            return f\"âŒ Error: {results.get('error', 'Unknown error')}\"\n",
    "        \n",
    "        output = f\"## ðŸ Benchmark Results: {results['benchmark']}\\n\\n\"\n",
    "        output += f\"**Category:** {results['category']}  \\n\"\n",
    "        output += f\"**Problem Size:** {results['size']:,}  \\n\\n\"\n",
    "        \n",
    "        # CPU Results\n",
    "        if results.get(\"cpu_result\"):\n",
    "            cpu_time = results[\"cpu_result\"].get(\"execution_time\", 0)\n",
    "            output += f\"### ðŸ–¥ï¸ CPU Performance (NumPy/Pandas/scikit-learn)\\n\"\n",
    "            output += f\"- **Execution Time:** {cpu_time:.4f} seconds\\n\"\n",
    "            \n",
    "            # Add additional metrics\n",
    "            for key, value in results[\"cpu_result\"].items():\n",
    "                if key != \"execution_time\":\n",
    "                    output += f\"- **{key.replace('_', ' ').title()}:** {value}\\n\"\n",
    "            output += \"\\n\"\n",
    "        \n",
    "        # GPU Results\n",
    "        if results.get(\"gpu_result\") and not results[\"gpu_result\"].get(\"error\"):\n",
    "            gpu_time = results[\"gpu_result\"].get(\"execution_time\", 0)\n",
    "            output += f\"### ðŸš€ GPU Performance (CuPy/cuDF/cuML)\\n\"\n",
    "            output += f\"- **Execution Time:** {gpu_time:.4f} seconds\\n\"\n",
    "            \n",
    "            # Add additional metrics\n",
    "            for key, value in results[\"gpu_result\"].items():\n",
    "                if key != \"execution_time\":\n",
    "                    output += f\"- **{key.replace('_', ' ').title()}:** {value}\\n\"\n",
    "            output += \"\\n\"\n",
    "            \n",
    "            # Speedup analysis\n",
    "            if results.get(\"speedup\"):\n",
    "                speedup = results[\"speedup\"]\n",
    "                winner = results[\"winner\"]\n",
    "                output += f\"### ðŸ“Š Performance Analysis\\n\"\n",
    "                output += f\"- **Speedup:** {speedup:.2f}x\\n\"\n",
    "                output += f\"- **Winner:** {winner} ðŸ†\\n\"\n",
    "                \n",
    "                if speedup > 1:\n",
    "                    output += f\"- **Performance Gain:** {((speedup - 1) * 100):.1f}% faster on GPU\\n\"\n",
    "                    if speedup > 10:\n",
    "                        output += \"- **Analysis:** ðŸ”¥ Excellent GPU acceleration! This workload is highly parallel.\\n\"\n",
    "                    elif speedup > 3:\n",
    "                        output += \"- **Analysis:** âœ… Good GPU performance gain.\\n\"\n",
    "                    else:\n",
    "                        output += \"- **Analysis:** âš¡ Moderate GPU acceleration.\\n\"\n",
    "                else:\n",
    "                    output += f\"- **Performance Loss:** {((1 - speedup) * 100):.1f}% slower on GPU\\n\"\n",
    "                    output += \"- **Analysis:** âš ï¸ GPU overhead dominates for this problem size. Try larger datasets.\\n\"\n",
    "        \n",
    "        elif results.get(\"gpu_result\", {}).get(\"error\"):\n",
    "            output += f\"### âŒ GPU Results\\n\"\n",
    "            output += f\"Error: {results['gpu_result']['error']}\\n\\n\"\n",
    "            output += \"ðŸ’¡ **Note:** GPU libraries (CuPy, cuDF, cuML) need to be installed for GPU benchmarks.\\n\"\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_recent_results(self, limit=5):\n",
    "        \"\"\"Get recent benchmark results.\"\"\"\n",
    "        return self.benchmark_results[-limit:] if self.benchmark_results else []\n",
    "\n",
    "# Initialize benchmark engine\n",
    "benchmark_engine = BenchmarkEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d470191c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EducationalContentEnhancer:\n",
    "    \"\"\"Enhance the RAG system with educational content and examples.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.code_examples = self._load_code_examples()\n",
    "        self.performance_insights = self._load_performance_insights()\n",
    "    \n",
    "    def _load_code_examples(self):\n",
    "        \"\"\"Load curated code examples for common GPU acceleration patterns.\"\"\"\n",
    "        return {\n",
    "            \"matrix_multiplication\": {\n",
    "                \"cpu_code\": \"\"\"\n",
    "# CPU Version with NumPy\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "size = 2048\n",
    "A = np.random.rand(size, size).astype(np.float32)\n",
    "B = np.random.rand(size, size).astype(np.float32)\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "C = np.matmul(A, B)\n",
    "cpu_time = time.perf_counter() - start_time\n",
    "print(f\"CPU time: {cpu_time:.4f} seconds\")\n",
    "\"\"\",\n",
    "                \"gpu_code\": \"\"\"\n",
    "# GPU Version with CuPy  \n",
    "import cupy as cp\n",
    "import time\n",
    "\n",
    "size = 2048\n",
    "A = cp.random.rand(size, size).astype(cp.float32)\n",
    "B = cp.random.rand(size, size).astype(cp.float32)\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "C = cp.matmul(A, B)\n",
    "cp.cuda.Device().synchronize()\n",
    "gpu_time = time.perf_counter() - start_time\n",
    "print(f\"GPU time: {gpu_time:.4f} seconds\")\n",
    "\"\"\",\n",
    "                \"expected_speedup\": \"10-50x\",\n",
    "                \"key_points\": [\n",
    "                    \"Use float32 for better GPU performance\",\n",
    "                    \"Synchronize GPU for accurate timing\",\n",
    "                    \"Performance scales with matrix size\"\n",
    "                ]\n",
    "            },\n",
    "            \n",
    "            \"dataframe_groupby\": {\n",
    "                \"cpu_code\": \"\"\"\n",
    "# CPU Version with Pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "n = 1000000\n",
    "df = pd.DataFrame({\n",
    "    'group': np.random.choice(['A', 'B', 'C', 'D'], n),\n",
    "    'value1': np.random.randn(n),\n",
    "    'value2': np.random.randn(n)\n",
    "})\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "result = df.groupby('group').agg({\n",
    "    'value1': ['mean', 'std'],\n",
    "    'value2': ['sum', 'count']\n",
    "})\n",
    "cpu_time = time.perf_counter() - start_time\n",
    "print(f\"CPU time: {cpu_time:.4f} seconds\")\n",
    "\"\"\",\n",
    "                \"gpu_code\": \"\"\"\n",
    "# GPU Version with cuDF\n",
    "import cudf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "n = 1000000\n",
    "df = cudf.DataFrame({\n",
    "    'group': np.random.choice(['A', 'B', 'C', 'D'], n),\n",
    "    'value1': np.random.randn(n),\n",
    "    'value2': np.random.randn(n)\n",
    "})\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "result = df.groupby('group').agg({\n",
    "    'value1': ['mean', 'std'],\n",
    "    'value2': ['sum', 'count']\n",
    "})\n",
    "gpu_time = time.perf_counter() - start_time\n",
    "print(f\"GPU time: {gpu_time:.4f} seconds\")\n",
    "\"\"\",\n",
    "                \"expected_speedup\": \"5-20x\",\n",
    "                \"key_points\": [\n",
    "                    \"cuDF API is nearly identical to pandas\",\n",
    "                    \"Best performance with large datasets\",\n",
    "                    \"GPU memory considerations for large DataFrames\"\n",
    "                ]\n",
    "            },\n",
    "            \n",
    "            \"element_wise_operations\": {\n",
    "                \"cpu_code\": \"\"\"\n",
    "# CPU Version - Element-wise operations\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "n = 10000000\n",
    "x = np.random.rand(n).astype(np.float32)\n",
    "y = np.random.rand(n).astype(np.float32)\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "# Multiple operations create intermediate arrays\n",
    "result = np.sqrt(x**2 + y**2)\n",
    "mean_result = np.mean(result)\n",
    "cpu_time = time.perf_counter() - start_time\n",
    "print(f\"CPU time: {cpu_time:.4f} seconds\")\n",
    "\"\"\",\n",
    "                \"gpu_code\": \"\"\"\n",
    "# GPU Version with CuPy and kernel fusion\n",
    "import cupy as cp\n",
    "import time\n",
    "\n",
    "n = 10000000\n",
    "x = cp.random.rand(n).astype(cp.float32)\n",
    "y = cp.random.rand(n).astype(cp.float32)\n",
    "\n",
    "@cp.fuse()\n",
    "def fused_distance_mean(x, y):\n",
    "    return cp.mean(cp.sqrt(x**2 + y**2))\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "mean_result = fused_distance_mean(x, y)\n",
    "cp.cuda.Device().synchronize()\n",
    "gpu_time = time.perf_counter() - start_time\n",
    "print(f\"GPU time: {gpu_time:.4f} seconds\")\n",
    "\"\"\",\n",
    "                \"expected_speedup\": \"3-15x\",\n",
    "                \"key_points\": [\n",
    "                    \"Use @cp.fuse() to reduce kernel launches\",\n",
    "                    \"Avoid creating unnecessary intermediate arrays\",\n",
    "                    \"Memory bandwidth often limits performance\"\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _load_performance_insights(self):\n",
    "        \"\"\"Load performance insights and optimization tips.\"\"\"\n",
    "        return {\n",
    "            \"general_principles\": [\n",
    "                \"GPU acceleration benefits scale with problem size\",\n",
    "                \"Memory bandwidth often bottlenecks GPU performance\",\n",
    "                \"Minimize CPU-GPU data transfers\",\n",
    "                \"Use appropriate data types (float32 vs float64)\",\n",
    "                \"Batch operations to amortize kernel launch overhead\"\n",
    "            ],\n",
    "            \n",
    "            \"when_to_use_gpu\": [\n",
    "                \"Large datasets (>100K elements for arrays, >50K rows for DataFrames)\",\n",
    "                \"Highly parallel operations (matrix multiplication, element-wise ops)\",\n",
    "                \"Repetitive computations that stay on GPU\",\n",
    "                \"Machine learning with large feature spaces\"\n",
    "            ],\n",
    "            \n",
    "            \"when_not_to_use_gpu\": [\n",
    "                \"Small datasets where overhead dominates\",\n",
    "                \"Sequential algorithms that don't parallelize well\",\n",
    "                \"Code with frequent CPU-GPU transfers\",\n",
    "                \"I/O bound operations\"\n",
    "            ],\n",
    "            \n",
    "            \"optimization_techniques\": {\n",
    "                \"cupy\": [\n",
    "                    \"Use @cp.fuse() for element-wise operations\",\n",
    "                    \"Keep data on GPU between operations\",\n",
    "                    \"Use streams for concurrent operations\",\n",
    "                    \"Profile with cupyx.profiler.benchmark\"\n",
    "                ],\n",
    "                \"cudf\": [\n",
    "                    \"Use appropriate dtypes to save memory\",\n",
    "                    \"Leverage GPU-accelerated string operations\",\n",
    "                    \"Use .query() for efficient filtering\",\n",
    "                    \"Batch operations on large DataFrames\"\n",
    "                ],\n",
    "                \"cuml\": [\n",
    "                    \"Use single precision (float32) when possible\",\n",
    "                    \"Leverage GPU memory for large datasets\",\n",
    "                    \"Use appropriate algorithm parameters\",\n",
    "                    \"Consider data preprocessing on GPU\"\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def get_example_for_operation(self, operation_type):\n",
    "        \"\"\"Get code example for a specific operation type.\"\"\"\n",
    "        return self.code_examples.get(operation_type, None)\n",
    "    \n",
    "    def get_optimization_tips(self, library):\n",
    "        \"\"\"Get optimization tips for a specific library.\"\"\"\n",
    "        return self.performance_insights.get(\"optimization_techniques\", {}).get(library, [])\n",
    "    \n",
    "    def get_performance_guidelines(self):\n",
    "        \"\"\"Get general performance guidelines.\"\"\"\n",
    "        return self.performance_insights\n",
    "\n",
    "# Initialize the educational content enhancer\n",
    "content_enhancer = EducationalContentEnhancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b654223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceVisualizer:\n",
    "    \"\"\"Create visualizations and insights for benchmark results.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.visualization_templates = self._setup_visualization_templates()\n",
    "    \n",
    "    def _setup_visualization_templates(self):\n",
    "        \"\"\"Setup templates for different types of performance visualizations.\"\"\"\n",
    "        return {\n",
    "            \"speedup_chart\": \"\"\"\n",
    "## ðŸ“ˆ Performance Speedup Analysis\n",
    "\n",
    "```\n",
    "Benchmark: {benchmark_name}\n",
    "Category: {category}\n",
    "Problem Size: {size:,}\n",
    "\n",
    "CPU Time:    {cpu_time:.4f}s  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
    "GPU Time:    {gpu_time:.4f}s  {gpu_bar}\n",
    "Speedup:     {speedup:.2f}x   {speedup_indicator}\n",
    "```\n",
    "\n",
    "**Performance Insights:**\n",
    "{insights}\n",
    "\"\"\",\n",
    "            \n",
    "            \"scaling_analysis\": \"\"\"\n",
    "## ðŸ“Š Performance Scaling Analysis\n",
    "\n",
    "**How performance changes with problem size:**\n",
    "\n",
    "{scaling_data}\n",
    "\n",
    "**Key Observations:**\n",
    "- GPU advantage increases with larger problem sizes\n",
    "- Overhead is more significant for smaller datasets\n",
    "- Memory bandwidth becomes the limiting factor at large scales\n",
    "\"\"\",\n",
    "            \n",
    "            \"comparison_matrix\": \"\"\"\n",
    "## ðŸ† Technology Comparison\n",
    "\n",
    "| Operation Type | CPU Library | GPU Library | Typical Speedup | Best Use Case |\n",
    "|----------------|-------------|-------------|-----------------|---------------|\n",
    "| Matrix Ops     | NumPy       | CuPy        | 10-50x         | Linear algebra, large arrays |\n",
    "| DataFrame Ops  | Pandas      | cuDF        | 5-20x          | Data processing, analytics |\n",
    "| ML Algorithms  | scikit-learn| cuML        | 5-25x          | Large datasets, feature engineering |\n",
    "| Math Functions | NumPy       | CuPy        | 3-15x          | Signal processing, numerical computing |\n",
    "\n",
    "**ðŸ’¡ Selection Guidelines:**\n",
    "- **Problem Size**: GPU benefits increase with larger datasets\n",
    "- **Memory**: Consider GPU memory limitations for very large data\n",
    "- **Pipeline**: Keep operations on GPU to avoid transfer overhead\n",
    "\"\"\"\n",
    "        }\n",
    "    \n",
    "    def create_speedup_visualization(self, benchmark_result):\n",
    "        \"\"\"Create a text-based speedup visualization.\"\"\"\n",
    "        if not benchmark_result or benchmark_result.get(\"error\"):\n",
    "            return \"âŒ No valid benchmark results to visualize\"\n",
    "        \n",
    "        cpu_result = benchmark_result.get(\"cpu_result\", {})\n",
    "        gpu_result = benchmark_result.get(\"gpu_result\", {})\n",
    "        \n",
    "        if not cpu_result or not gpu_result or gpu_result.get(\"error\"):\n",
    "            return \"âŒ Incomplete benchmark results for visualization\"\n",
    "        \n",
    "        cpu_time = cpu_result.get(\"execution_time\", 0)\n",
    "        gpu_time = gpu_result.get(\"execution_time\", 0)\n",
    "        speedup = benchmark_result.get(\"speedup\", 1)\n",
    "        \n",
    "        # Create simple text bar visualization\n",
    "        max_bar_length = 24\n",
    "        if cpu_time > 0:\n",
    "            gpu_bar_length = max(1, int((gpu_time / cpu_time) * max_bar_length))\n",
    "            gpu_bar = \"â–ˆ\" * gpu_bar_length\n",
    "        else:\n",
    "            gpu_bar = \"â–ˆ\"\n",
    "        \n",
    "        # Speedup indicator\n",
    "        if speedup > 10:\n",
    "            speedup_indicator = \"ðŸ”¥ Excellent acceleration!\"\n",
    "        elif speedup > 3:\n",
    "            speedup_indicator = \"âœ… Good performance gain\"\n",
    "        elif speedup > 1:\n",
    "            speedup_indicator = \"âš¡ Moderate improvement\"\n",
    "        else:\n",
    "            speedup_indicator = \"âš ï¸ GPU overhead dominates\"\n",
    "        \n",
    "        # Generate insights\n",
    "        insights = self._generate_performance_insights(benchmark_result)\n",
    "        \n",
    "        return self.visualization_templates[\"speedup_chart\"].format(\n",
    "            benchmark_name=benchmark_result.get(\"benchmark\", \"Unknown\"),\n",
    "            category=benchmark_result.get(\"category\", \"Unknown\"),\n",
    "            size=benchmark_result.get(\"size\", 0),\n",
    "            cpu_time=cpu_time,\n",
    "            gpu_time=gpu_time,\n",
    "            gpu_bar=gpu_bar,\n",
    "            speedup=speedup,\n",
    "            speedup_indicator=speedup_indicator,\n",
    "            insights=insights\n",
    "        )\n",
    "    \n",
    "    def _generate_performance_insights(self, benchmark_result):\n",
    "        \"\"\"Generate specific insights based on benchmark results.\"\"\"\n",
    "        insights = []\n",
    "        speedup = benchmark_result.get(\"speedup\", 1)\n",
    "        category = benchmark_result.get(\"category\", \"\")\n",
    "        size = benchmark_result.get(\"size\", 0)\n",
    "        \n",
    "        # Size-based insights\n",
    "        if size < 1000:\n",
    "            insights.append(\"â€¢ Small problem size - GPU overhead may limit benefits\")\n",
    "        elif size < 100000:\n",
    "            insights.append(\"â€¢ Medium problem size - good balance of performance and overhead\")\n",
    "        else:\n",
    "            insights.append(\"â€¢ Large problem size - excellent candidate for GPU acceleration\")\n",
    "        \n",
    "        # Category-specific insights\n",
    "        if \"Matrix\" in category:\n",
    "            if speedup > 10:\n",
    "                insights.append(\"â€¢ Matrix operations scale excellently on GPU due to high parallelism\")\n",
    "            else:\n",
    "                insights.append(\"â€¢ Consider larger matrices or float32 data type for better GPU performance\")\n",
    "        \n",
    "        elif \"DataFrame\" in category:\n",
    "            if speedup > 5:\n",
    "                insights.append(\"â€¢ DataFrame operations benefit from GPU's high memory bandwidth\")\n",
    "            else:\n",
    "                insights.append(\"â€¢ Try larger datasets or more complex operations for better GPU utilization\")\n",
    "        \n",
    "        elif \"Machine Learning\" in category:\n",
    "            if speedup > 5:\n",
    "                insights.append(\"â€¢ ML algorithms show good GPU acceleration with parallel computations\")\n",
    "            else:\n",
    "                insights.append(\"â€¢ Consider hyperparameter tuning or larger feature spaces\")\n",
    "        \n",
    "        # Performance-based insights\n",
    "        if speedup < 1:\n",
    "            insights.append(\"â€¢ GPU overhead exceeds benefits - consider CPU for this workload\")\n",
    "        elif speedup > 20:\n",
    "            insights.append(\"â€¢ Exceptional GPU performance - this workload is highly parallel\")\n",
    "        \n",
    "        return \"\\n\".join(insights) if insights else \"â€¢ Standard GPU acceleration performance\"\n",
    "    \n",
    "    def create_educational_summary(self, benchmark_result):\n",
    "        \"\"\"Create educational summary explaining the results.\"\"\"\n",
    "        category = benchmark_result.get(\"category\", \"Unknown\")\n",
    "        speedup = benchmark_result.get(\"speedup\", 1)\n",
    "        \n",
    "        summary = f\"\"\"\n",
    "### ðŸŽ“ Educational Summary\n",
    "\n",
    "**What happened in this benchmark:**\n",
    "\"\"\"\n",
    "        \n",
    "        if \"Matrix\" in category:\n",
    "            summary += \"\"\"\n",
    "1. **CPU Processing**: NumPy used optimized BLAS libraries but was limited by sequential processing\n",
    "2. **GPU Processing**: CuPy leveraged thousands of CUDA cores for parallel matrix computations\n",
    "3. **Key Factor**: Matrix multiplication is embarrassingly parallel, ideal for GPU architecture\n",
    "\"\"\"\n",
    "        \n",
    "        elif \"DataFrame\" in category:\n",
    "            summary += \"\"\"\n",
    "1. **CPU Processing**: Pandas processed data sequentially with some multi-threading\n",
    "2. **GPU Processing**: cuDF utilized GPU's high memory bandwidth and parallel cores\n",
    "3. **Key Factor**: GroupBy operations benefit from GPU's ability to process many groups simultaneously\n",
    "\"\"\"\n",
    "        \n",
    "        elif \"Machine Learning\" in category:\n",
    "            summary += \"\"\"\n",
    "1. **CPU Processing**: scikit-learn used optimized CPU algorithms\n",
    "2. **GPU Processing**: cuML leveraged GPU parallelism for distance calculations and updates\n",
    "3. **Key Factor**: ML algorithms with many data points benefit from massive parallelization\n",
    "\"\"\"\n",
    "        \n",
    "        # Add learning objectives\n",
    "        summary += f\"\"\"\n",
    "**Learning Objectives Achieved:**\n",
    "â€¢ Demonstrated {speedup:.1f}x performance improvement with GPU acceleration\n",
    "â€¢ Showed real-world application of NVIDIA Rapids ecosystem\n",
    "â€¢ Illustrated when GPU acceleration provides significant benefits\n",
    "â€¢ Experienced hands-on performance comparison\n",
    "\n",
    "**Next Steps to Explore:**\n",
    "â€¢ Try different problem sizes to see how speedup scales\n",
    "â€¢ Experiment with different data types (float32 vs float64)\n",
    "â€¢ Explore memory usage patterns between CPU and GPU implementations\n",
    "\"\"\"\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Initialize performance visualizer\n",
    "perf_visualizer = PerformanceVisualizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3052fad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import json\n",
    "\n",
    "def chat_with_mentor(message, code, chat_history):\n",
    "    \"\"\"Handle chat interactions with the GPU Mentor - now integrates code with LLM.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Process user input through the enhanced mentor (code + message together)\n",
    "        response = gpu_mentor.process_user_input(message, code)\n",
    "        \n",
    "        # Format response for chat\n",
    "        formatted_response = response[\"text_response\"]\n",
    "        \n",
    "        # Add code analysis if available\n",
    "        if response[\"code_analysis\"]:\n",
    "            analysis = response[\"code_analysis\"]\n",
    "            formatted_response += f\"\\n\\n**ðŸ“Š Code Analysis:**\\n\"\n",
    "            formatted_response += f\"â€¢ Libraries detected: {', '.join(analysis['libraries_detected'])}\\n\"\n",
    "            formatted_response += f\"â€¢ Estimated speedup potential: {analysis['estimated_speedup']:.1f}x\\n\"\n",
    "            formatted_response += f\"â€¢ GPU compatible: {'âœ…' if analysis['gpu_compatible'] else 'âŒ'}\\n\"\n",
    "            \n",
    "            if analysis['warnings']:\n",
    "                formatted_response += f\"â€¢ âš ï¸ Warnings: {'; '.join(analysis['warnings'])}\\n\"\n",
    "        \n",
    "        # Add code execution output\n",
    "        if response[\"code_output\"]:\n",
    "            output = response[\"code_output\"]\n",
    "            formatted_response += f\"\\n\\n**âš¡ Code Execution Results:**\\n\"\n",
    "            \n",
    "            if output.get(\"status\") == \"success\":\n",
    "                formatted_response += f\"â€¢ âœ… Execution successful ({output.get('execution_time', 0):.3f}s)\\n\"\n",
    "                \n",
    "                if output.get(\"stdout\"):\n",
    "                    formatted_response += f\"â€¢ ðŸ“„ Output:\\n```\\n{output['stdout']}\\n```\\n\"\n",
    "                \n",
    "                if output.get(\"variables\"):\n",
    "                    formatted_response += f\"â€¢ ðŸ“Š Variables created: {', '.join(output['variables'].keys())}\\n\"\n",
    "                    # Show details for important variables\n",
    "                    for var_name, var_info in list(output['variables'].items())[:3]:\n",
    "                        formatted_response += f\"  - `{var_name}`: {var_info}\\n\"\n",
    "            else:\n",
    "                formatted_response += f\"â€¢ âŒ Execution failed: {output.get('error', 'Unknown error')}\\n\"\n",
    "                if output.get(\"stderr\"):\n",
    "                    formatted_response += f\"â€¢ ðŸš¨ Error details:\\n```\\n{output['stderr']}\\n```\\n\"\n",
    "        \n",
    "        # Add Socratic questions\n",
    "        if response[\"socratic_questions\"]:\n",
    "            formatted_response += f\"\\n\\n**ðŸ¤” Think About This:**\\n\"\n",
    "            for i, question in enumerate(response[\"socratic_questions\"], 1):\n",
    "                formatted_response += f\"{i}. {question}\\n\"\n",
    "        \n",
    "        # Update chat history\n",
    "        if chat_history is None:\n",
    "            chat_history = []\n",
    "        \n",
    "        # Format user message with code if provided\n",
    "        user_message = message\n",
    "        if code and code.strip():\n",
    "            user_message += f\"\\n\\n```python\\n{code}\\n```\"\n",
    "        \n",
    "        chat_history.append({\"role\": \"user\", \"content\": user_message})\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": formatted_response})\n",
    "        \n",
    "        return \"\", \"\", chat_history, response.get(\"code_output\"), response.get(\"optimized_code\", \"\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"âŒ Error: {str(e)}\"\n",
    "        if chat_history is None:\n",
    "            chat_history = []\n",
    "        chat_history.append({\"role\": \"user\", \"content\": message})\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": error_msg})\n",
    "        return \"\", \"\", chat_history, None, \"\"\n",
    "\n",
    "def analyze_code_only(code):\n",
    "    \"\"\"Analyze code for optimization opportunities.\"\"\"\n",
    "    \n",
    "    if not code.strip():\n",
    "        return \"Please provide code to analyze.\", \"\"\n",
    "    \n",
    "    try:\n",
    "        analysis = code_optimizer.analyze_code(code)\n",
    "        optimized_code = code_optimizer.suggest_optimizations(code)\n",
    "        \n",
    "        analysis_text = f\"\"\"\n",
    "**ðŸ” Code Analysis Results:**\n",
    "â€¢ Libraries detected: {', '.join(analysis['libraries_detected'])}\n",
    "â€¢ Estimated speedup potential: {analysis['estimated_speedup']:.1f}x\n",
    "â€¢ GPU compatible: {'âœ… Yes' if analysis['gpu_compatible'] else 'âŒ No'}\n",
    "\n",
    "**âš¡ Optimization Opportunities:**\n",
    "â€¢ Matrix operations: {'âœ… Detected' if any(op in code for op in ['np.dot', 'np.matmul', '@']) else 'âŒ None'}\n",
    "â€¢ Array operations: {'âœ… Detected' if 'numpy' in analysis['libraries_detected'] else 'âŒ None'}\n",
    "â€¢ DataFrame operations: {'âœ… Detected' if 'pandas' in analysis['libraries_detected'] else 'âŒ None'}\n",
    "â€¢ Loop vectorization: {'âœ… Possible' if 'for ' in code and 'range(' in code else 'âŒ None'}\n",
    "\n",
    "**âš ï¸ Considerations:**\n",
    "{chr(10).join('â€¢ ' + warning for warning in analysis['warnings']) if analysis['warnings'] else 'â€¢ None detected'}\n",
    "\n",
    "**ðŸ’¡ Recommendations:**\n",
    "â€¢ Consider using CuPy for NumPy operations on large arrays\n",
    "â€¢ Try cuDF for pandas operations on large datasets  \n",
    "â€¢ Use memory pools for repeated GPU operations\n",
    "â€¢ Profile memory usage for optimal batch sizes\n",
    "\"\"\"\n",
    "        \n",
    "        return analysis_text, optimized_code\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error analyzing code: {str(e)}\", \"\"\n",
    "\n",
    "def get_tutorial(topic):\n",
    "    \"\"\"Generate tutorial content for specific topics.\"\"\"\n",
    "    \n",
    "    if not topic.strip():\n",
    "        return \"Please specify a topic for the tutorial.\"\n",
    "    \n",
    "    try:\n",
    "        tutorial_content = gpu_mentor.generate_tutorial_content(topic)\n",
    "        return tutorial_content\n",
    "    except Exception as e:\n",
    "        return f\"Error generating tutorial: {str(e)}\"\n",
    "\n",
    "def clear_chat():\n",
    "    \"\"\"Clear chat history.\"\"\"\n",
    "    return None, None, None, None, \"\"\n",
    "\n",
    "# Sample code examples for quick testing\n",
    "sample_codes = {\n",
    "    \"Simple Array Operations\": '''import numpy as np\n",
    "\n",
    "# Create arrays\n",
    "n = 1000\n",
    "x = np.random.rand(n)\n",
    "y = np.random.rand(n)\n",
    "\n",
    "# Basic operations\n",
    "result = np.sqrt(x**2 + y**2)\n",
    "mean_result = np.mean(result)\n",
    "\n",
    "print(f\"Array size: {n}\")\n",
    "print(f\"Mean result: {mean_result:.4f}\")\n",
    "print(f\"Max result: {np.max(result):.4f}\")''',\n",
    "\n",
    "    \"Matrix Multiplication\": '''import numpy as np\n",
    "\n",
    "# Create matrices\n",
    "n = 500\n",
    "A = np.random.rand(n, n)\n",
    "B = np.random.rand(n, n)\n",
    "\n",
    "# Matrix multiplication\n",
    "C = np.dot(A, B)\n",
    "\n",
    "print(f\"Matrix size: {n}x{n}\")\n",
    "print(f\"Result shape: {C.shape}\")\n",
    "print(f\"Result sum: {np.sum(C):.2f}\")''',\n",
    "    \n",
    "    \"DataFrame Operations\": '''import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create dataset\n",
    "n = 10000\n",
    "df = pd.DataFrame({\n",
    "    'x': np.random.randn(n),\n",
    "    'y': np.random.randn(n),\n",
    "    'group': np.random.choice(['A', 'B', 'C'], n)\n",
    "})\n",
    "\n",
    "# Compute statistics\n",
    "result = df.groupby('group').agg({\n",
    "    'x': ['mean', 'std'],\n",
    "    'y': ['sum', 'count']\n",
    "})\n",
    "\n",
    "print(f\"Dataset size: {len(df)} rows\")\n",
    "print(\"Grouped results:\")\n",
    "print(result)''',\n",
    "    \n",
    "    \"Mathematical Functions\": '''import numpy as np\n",
    "\n",
    "# Generate data\n",
    "n = 5000\n",
    "x = np.linspace(0, 4*np.pi, n)\n",
    "y = np.sin(x) * np.exp(-x/10)\n",
    "\n",
    "# Compute statistics\n",
    "mean_y = np.mean(y)\n",
    "std_y = np.std(y)\n",
    "max_y = np.max(y)\n",
    "\n",
    "print(f\"Data points: {n}\")\n",
    "print(f\"Mean: {mean_y:.4f}\")\n",
    "print(f\"Std: {std_y:.4f}\")\n",
    "print(f\"Max: {max_y:.4f}\")''',\n",
    "\n",
    "    \"Data Processing Loop\": '''import numpy as np\n",
    "\n",
    "# Create data\n",
    "data = np.random.rand(1000, 10)\n",
    "results = []\n",
    "\n",
    "# Process data (can be vectorized)\n",
    "for i in range(len(data)):\n",
    "    row_sum = np.sum(data[i])\n",
    "    row_mean = np.mean(data[i])\n",
    "    results.append(row_sum * row_mean)\n",
    "\n",
    "final_result = np.array(results)\n",
    "print(f\"Processed {len(data)} rows\")\n",
    "print(f\"Final result shape: {final_result.shape}\")\n",
    "print(f\"Average result: {np.mean(final_result):.4f}\")'''\n",
    "}\n",
    "\n",
    "# Create the enhanced Gradio interface\n",
    "with gr.Blocks(title=\"GPU Mentor - Enhanced AI Tutor\", theme=gr.themes.Soft()) as demo:\n",
    "    \n",
    "    gr.Markdown(\"# ðŸš€ Enhanced GPU Mentor: AI Tutor with Integrated Code Execution\")\n",
    "    \n",
    "    with gr.Tabs():\n",
    "        \n",
    "        # Features Tab (moved from main interface)\n",
    "        with gr.Tab(\"â„¹ï¸ Features\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            ## ðŸš€ Enhanced GPU Mentor Features\n",
    "            \n",
    "            **ðŸ”— Integrated LLM + Code**: Ask questions about your code - the AI sees both your question and code together\n",
    "            \n",
    "            **âš¡ Live Code Execution**: Run your Python code instantly and see the output in the chat\n",
    "            \n",
    "            **ðŸ” Smart Analysis**: Get optimization suggestions and GPU acceleration opportunities\n",
    "            \n",
    "            **ðŸ“š Educational Guidance**: Socratic questions and learning objectives based on your actual code\n",
    "            \n",
    "            **ðŸŽ¯ Multi-Modal Support**: Handles text questions, code analysis, and execution all in one interface\n",
    "            \n",
    "            **ðŸš€ GPU Optimization**: Automatic detection of optimization opportunities and GPU-compatible code suggestions\n",
    "            \n",
    "            **ðŸ“Š Performance Insights**: Real-time analysis of code performance and potential speedup estimates\n",
    "            \"\"\")\n",
    "        \n",
    "        # Main Chat Playground Tab (Redesigned)\n",
    "        with gr.Tab(\"ðŸ’¬ Chat Playground\"):\n",
    "            with gr.Column():\n",
    "                # Main conversation area\n",
    "                chatbot = gr.Chatbot(\n",
    "                    label=\"GPU Mentor Conversation\",\n",
    "                    height=500,\n",
    "                    type=\"messages\",\n",
    "                    show_copy_button=True\n",
    "                )\n",
    "                \n",
    "                # Integrated input area at bottom of conversation\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=3):\n",
    "                        message_input = gr.Textbox(\n",
    "                            label=\"\",\n",
    "                            placeholder=\"Ask about GPU acceleration, optimization, or explain your code...\",\n",
    "                            lines=2,\n",
    "                            show_label=False\n",
    "                        )\n",
    "                    with gr.Column(scale=1):\n",
    "                        submit_btn = gr.Button(\"ðŸ’¬ Send\", variant=\"primary\", size=\"lg\")\n",
    "                        clear_btn = gr.Button(\"ðŸ§¹ Clear\", size=\"sm\")\n",
    "                \n",
    "                # Code input area (collapsible)\n",
    "                with gr.Accordion(\"ðŸ“ Python Code (Optional)\", open=False):\n",
    "                    code_input = gr.Code(\n",
    "                        label=\"\",\n",
    "                        language=\"python\",\n",
    "                        lines=8,\n",
    "                        show_label=False,\n",
    "                        # placeholder=\"# Enter your Python code here (optional)\\n# The AI will analyze and execute it along with your question\"\n",
    "                    )\n",
    "                    \n",
    "                    # Sample code selector\n",
    "                    with gr.Row():\n",
    "                        sample_dropdown = gr.Dropdown(\n",
    "                            choices=list(sample_codes.keys()),\n",
    "                            label=\"Load Sample Code\",\n",
    "                            value=None,\n",
    "                            scale=2\n",
    "                        )\n",
    "                        load_sample_btn = gr.Button(\"ðŸ“‚ Load\", scale=1)\n",
    "        \n",
    "        # Code Analysis Tab (Updated)\n",
    "        with gr.Tab(\"ðŸ” Code Analysis & Optimization\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    analyze_code = gr.Code(\n",
    "                        label=\"Code to Analyze\",\n",
    "                        language=\"python\",\n",
    "                        lines=15\n",
    "                    )\n",
    "                    \n",
    "                    analyze_btn = gr.Button(\"ðŸ” Analyze Code\", variant=\"primary\")\n",
    "                    \n",
    "                    analysis_results = gr.Textbox(\n",
    "                        label=\"Analysis Results\",\n",
    "                        lines=15\n",
    "                    )\n",
    "                \n",
    "                with gr.Column():\n",
    "                    optimized_code = gr.Code(\n",
    "                        label=\"GPU-Optimized Version\",\n",
    "                        language=\"python\",\n",
    "                        lines=20\n",
    "                    )\n",
    "        \n",
    "        # Performance Benchmarking Tab (NEW)\n",
    "        with gr.Tab(\"ðŸ Performance Benchmarking\"):\n",
    "            gr.Markdown(\"## ðŸš€ CPU vs GPU Performance Comparison\")\n",
    "            gr.Markdown(\"\"\"\n",
    "            Compare the performance of CPU and GPU implementations across different workloads.\n",
    "            This interactive benchmarking tool demonstrates real-world GPU acceleration benefits.\n",
    "            \"\"\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    # Benchmark selection controls\n",
    "                    benchmark_category = gr.Dropdown(\n",
    "                        choices=benchmark_engine.get_benchmark_categories(),\n",
    "                        label=\"ðŸ“‚ Benchmark Category\",\n",
    "                        value=\"Matrix Operations\"\n",
    "                    )\n",
    "                    \n",
    "                    benchmark_name = gr.Dropdown(\n",
    "                        choices=[],\n",
    "                        label=\"ðŸŽ¯ Specific Benchmark\",\n",
    "                        value=None\n",
    "                    )\n",
    "                    \n",
    "                    benchmark_size = gr.Dropdown(\n",
    "                        choices=[],\n",
    "                        label=\"ðŸ“ Problem Size\",\n",
    "                        value=None\n",
    "                    )\n",
    "                    \n",
    "                    run_benchmark_btn = gr.Button(\"ðŸƒâ€â™‚ï¸ Run Benchmark\", variant=\"primary\", size=\"lg\")\n",
    "                    \n",
    "                    # Benchmark status\n",
    "                    benchmark_status = gr.Textbox(\n",
    "                        label=\"Status\",\n",
    "                        value=\"Select benchmark parameters and click 'Run Benchmark'\",\n",
    "                        interactive=False,\n",
    "                        lines=2\n",
    "                    )\n",
    "                \n",
    "                with gr.Column(scale=2):\n",
    "                    # Results display\n",
    "                    benchmark_results = gr.Markdown(\n",
    "                        label=\"ðŸ“Š Benchmark Results\",\n",
    "                        value=\"\"\"\n",
    "### ðŸŽ¯ Ready to Benchmark!\n",
    "\n",
    "Select a category, benchmark, and problem size from the left panel, then click **Run Benchmark** to see CPU vs GPU performance comparison.\n",
    "\n",
    "**Available Categories:**\n",
    "- **Matrix Operations**: Linear algebra operations (NumPy vs CuPy)\n",
    "- **DataFrame Operations**: Data processing tasks (Pandas vs cuDF)  \n",
    "- **Machine Learning**: ML algorithms (scikit-learn vs cuML)\n",
    "- **Mathematical Functions**: Mathematical computations (NumPy vs CuPy)\n",
    "\n",
    "**What You'll Learn:**\n",
    "- Real-world GPU acceleration benefits\n",
    "- Performance scaling with problem size\n",
    "- When GPU acceleration is most effective\n",
    "- Memory and computational trade-offs\n",
    "\n",
    "**ðŸ’¡ Pro Tips:**\n",
    "- Start with Matrix Operations for dramatic speedups\n",
    "- DataFrame Operations work best with large datasets (>100K rows)\n",
    "- ML Algorithms show consistent benefits across problem sizes\n",
    "- Mathematical Functions benefit from kernel fusion techniques\n",
    "\"\"\"\n",
    "                    )\n",
    "            \n",
    "            # Technology comparison section\n",
    "            with gr.Row():\n",
    "                gr.Markdown(\"\"\"\n",
    "### ðŸ† Technology Comparison Guide\n",
    "\n",
    "| **Operation Type** | **CPU Library** | **GPU Library** | **Typical Speedup** | **Best Use Case** |\n",
    "|-------------------|-----------------|-----------------|---------------------|-------------------|\n",
    "| **Matrix Operations** | NumPy | CuPy | 10-50x | Linear algebra, large arrays |\n",
    "| **DataFrame Operations** | Pandas | cuDF | 5-20x | Data processing, analytics |\n",
    "| **ML Algorithms** | scikit-learn | cuML | 5-25x | Large datasets, feature engineering |\n",
    "| **Math Functions** | NumPy | CuPy | 3-15x | Signal processing, numerical computing |\n",
    "\n",
    "**ðŸŽ¯ Selection Guidelines:**\n",
    "- **Problem Size**: GPU benefits increase with larger datasets\n",
    "- **Memory**: Consider GPU memory limitations for very large data  \n",
    "- **Pipeline**: Keep operations on GPU to avoid transfer overhead\n",
    "- **Data Type**: Use float32 when possible for better GPU performance\n",
    "\"\"\")\n",
    "            \n",
    "            # Recent benchmarks section\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    gr.Markdown(\"### ðŸ“ˆ Recent Benchmark History\")\n",
    "                    recent_benchmarks_btn = gr.Button(\"ðŸ” Show Recent Results\")\n",
    "                    recent_benchmarks = gr.JSON(label=\"Recent Benchmarks\", visible=False)\n",
    "                    \n",
    "                    # Quick benchmark buttons for common operations\n",
    "                    gr.Markdown(\"### âš¡ Quick Benchmarks\")\n",
    "                    with gr.Row():\n",
    "                        quick_matrix_btn = gr.Button(\"Matrix Ops\", size=\"sm\")\n",
    "                        quick_dataframe_btn = gr.Button(\"DataFrame Ops\", size=\"sm\") \n",
    "                        quick_ml_btn = gr.Button(\"ML Algorithms\", size=\"sm\")\n",
    "                        quick_math_btn = gr.Button(\"Math Functions\", size=\"sm\")\n",
    "        \n",
    "        # Tutorial Generator Tab  \n",
    "        with gr.Tab(\"ðŸ“š Personalized Tutorials\"):\n",
    "            with gr.Column():\n",
    "                tutorial_topic = gr.Textbox(\n",
    "                    label=\"Tutorial Topic\",\n",
    "                    placeholder=\"e.g., 'CuPy memory management', 'cuDF vs pandas performance', 'vectorizing loops'...\",\n",
    "                    lines=1\n",
    "                )\n",
    "                \n",
    "                generate_tutorial_btn = gr.Button(\"ðŸ“ Generate Tutorial\", variant=\"primary\")\n",
    "                \n",
    "                tutorial_content = gr.Markdown(\n",
    "                    label=\"Tutorial Content\",\n",
    "                    value=\"Enter a topic above to generate a personalized tutorial.\"\n",
    "                )\n",
    "        \n",
    "        # Execution Summary Tab (Replaces Performance)\n",
    "        with gr.Tab(\"ðŸ“ˆ Code Execution Summary\"):\n",
    "            with gr.Column():\n",
    "                gr.Markdown(\"### Your Code Execution History\")\n",
    "                \n",
    "                summary_btn = gr.Button(\"ðŸ“Š View Execution Summary\")\n",
    "                execution_summary = gr.JSON(label=\"Execution Summary\")\n",
    "    \n",
    "    \n",
    "    # Supporting functions for interface\n",
    "    def get_tutorial(topic):\n",
    "        \"\"\"Generate tutorial content for the given topic.\"\"\"\n",
    "        if not topic.strip():\n",
    "            return \"Please enter a topic to generate a tutorial.\"\n",
    "        \n",
    "        try:\n",
    "            content = gpu_mentor.generate_tutorial_content(topic)\n",
    "            return content\n",
    "        except Exception as e:\n",
    "            return f\"Error generating tutorial: {str(e)}\"\n",
    "    \n",
    "    def clear_chat():\n",
    "        \"\"\"Clear the chat history.\"\"\"\n",
    "        return [], None, None\n",
    "    \n",
    "    # Benchmarking functions\n",
    "    def update_benchmark_options(category):\n",
    "        \"\"\"Update benchmark name dropdown based on selected category.\"\"\"\n",
    "        if not category:\n",
    "            return gr.Dropdown(choices=[], value=None), gr.Dropdown(choices=[], value=None)\n",
    "        \n",
    "        benchmarks = benchmark_engine.get_benchmarks_for_category(category)\n",
    "        return (\n",
    "            gr.Dropdown(choices=benchmarks, value=benchmarks[0] if benchmarks else None),\n",
    "            gr.Dropdown(choices=[], value=None)\n",
    "        )\n",
    "    \n",
    "    def update_size_options(category, benchmark_name):\n",
    "        \"\"\"Update size dropdown based on selected benchmark.\"\"\"\n",
    "        if not category or not benchmark_name:\n",
    "            return gr.Dropdown(choices=[], value=None)\n",
    "        \n",
    "        sizes = benchmark_engine.get_benchmark_sizes(category, benchmark_name)\n",
    "        return gr.Dropdown(choices=sizes, value=sizes[0] if sizes else None)\n",
    "    \n",
    "    def run_selected_benchmark(category, benchmark_name, size):\n",
    "        \"\"\"Run the selected benchmark and return enhanced results with visualizations.\"\"\"\n",
    "        if not all([category, benchmark_name, size]):\n",
    "            return \"âŒ Please select all benchmark parameters\", \"Please select category, benchmark, and size\"\n",
    "        \n",
    "        status_msg = f\"ðŸƒâ€â™‚ï¸ Running {benchmark_name} benchmark with size {size:,}...\"\n",
    "        \n",
    "        try:\n",
    "            results = benchmark_engine.run_benchmark(category, benchmark_name, size)\n",
    "            \n",
    "            if not results:\n",
    "                return \"âŒ Benchmark failed to execute\", \"Benchmark execution failed\"\n",
    "            \n",
    "            # Create comprehensive results with visualizations\n",
    "            formatted_results = \"\"\n",
    "            \n",
    "            # Add basic benchmark results\n",
    "            basic_results = benchmark_engine.format_benchmark_results(results)\n",
    "            formatted_results += basic_results\n",
    "            \n",
    "            # Add performance visualization if successful\n",
    "            if results.get(\"speedup\") and not results.get(\"error\"):\n",
    "                formatted_results += \"\\n\\n---\\n\\n\"\n",
    "                formatted_results += perf_visualizer.create_speedup_visualization(results)\n",
    "                \n",
    "                # Add educational summary\n",
    "                formatted_results += \"\\n\\n---\\n\\n\"\n",
    "                formatted_results += perf_visualizer.create_educational_summary(results)\n",
    "                \n",
    "                # Add code examples for this category\n",
    "                formatted_results += \"\\n\\n---\\n\\n\"\n",
    "                formatted_results += f\"### ðŸ’» Code Examples\\n\\n\"\n",
    "                \n",
    "                if \"Matrix\" in category:\n",
    "                    example = content_enhancer.get_example_for_operation(\"matrix_multiplication\")\n",
    "                elif \"DataFrame\" in category:\n",
    "                    example = content_enhancer.get_example_for_operation(\"dataframe_groupby\")\n",
    "                else:\n",
    "                    example = content_enhancer.get_example_for_operation(\"element_wise_operations\")\n",
    "                \n",
    "                if example:\n",
    "                    formatted_results += f\"**CPU Implementation:**\\n```python\\n{example['cpu_code']}\\n```\\n\\n\"\n",
    "                    formatted_results += f\"**GPU Implementation:**\\n```python\\n{example['gpu_code']}\\n```\\n\\n\"\n",
    "                    formatted_results += f\"**Expected Speedup:** {example['expected_speedup']}\\n\\n\"\n",
    "                    formatted_results += \"**Key Optimization Points:**\\n\"\n",
    "                    for point in example['key_points']:\n",
    "                        formatted_results += f\"â€¢ {point}\\n\"\n",
    "            \n",
    "            # Success message\n",
    "            if results and results.get(\"speedup\"):\n",
    "                success_msg = f\"âœ… Completed! GPU Speedup: {results['speedup']:.2f}x\"\n",
    "                if results['speedup'] > 10:\n",
    "                    success_msg += \" ðŸ”¥ Excellent acceleration!\"\n",
    "                elif results['speedup'] > 3:\n",
    "                    success_msg += \" âœ… Good performance!\"\n",
    "                elif results['speedup'] > 1:\n",
    "                    success_msg += \" âš¡ Moderate improvement\"\n",
    "                else:\n",
    "                    success_msg += \" âš ï¸ GPU overhead detected\"\n",
    "            else:\n",
    "                success_msg = \"âœ… Benchmark completed\"\n",
    "                \n",
    "            return formatted_results, success_msg\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"âŒ Benchmark failed: {str(e)}\"\n",
    "            error_details = f\"\"\"\n",
    "## âŒ Benchmark Error\n",
    "\n",
    "**Error Details:** {str(e)}\n",
    "\n",
    "**Possible Causes:**\n",
    "â€¢ GPU libraries (CuPy, cuDF, cuML) may not be installed\n",
    "â€¢ Insufficient GPU memory for the selected problem size\n",
    "â€¢ CUDA driver/runtime issues\n",
    "\n",
    "**Troubleshooting:**\n",
    "â€¢ Try a smaller problem size\n",
    "â€¢ Check GPU memory availability\n",
    "â€¢ Verify RAPIDS installation: `conda list | grep -E \"(cupy|cudf|cuml)\"`\n",
    "\n",
    "**Note:** CPU benchmarks should still work even without GPU libraries.\n",
    "\"\"\"\n",
    "            return error_details, error_msg\n",
    "    \n",
    "    def show_recent_benchmarks():\n",
    "        \"\"\"Show recent benchmark results.\"\"\"\n",
    "        recent = benchmark_engine.get_recent_results()\n",
    "        if not recent:\n",
    "            return {\"message\": \"No recent benchmarks\"}, gr.JSON(visible=True)\n",
    "        \n",
    "        summary = []\n",
    "        for result in recent:\n",
    "            summary.append({\n",
    "                \"benchmark\": result.get(\"benchmark\", \"Unknown\"),\n",
    "                \"category\": result.get(\"category\", \"Unknown\"),\n",
    "                \"size\": result.get(\"size\", 0),\n",
    "                \"speedup\": result.get(\"speedup\", \"N/A\"),\n",
    "                \"winner\": result.get(\"winner\", \"N/A\")\n",
    "            })\n",
    "        \n",
    "        return summary, gr.JSON(visible=True)\n",
    "    \n",
    "    def run_quick_benchmark(benchmark_type):\n",
    "        \"\"\"Run a quick benchmark for common operations.\"\"\"\n",
    "        quick_benchmarks = {\n",
    "            \"Matrix Ops\": (\"Matrix Operations\", \"Matrix Multiplication\", 1024),\n",
    "            \"DataFrame Ops\": (\"DataFrame Operations\", \"GroupBy Aggregation\", 500000),\n",
    "            \"ML Algorithms\": (\"Machine Learning\", \"K-Means Clustering\", 50000),\n",
    "            \"Math Functions\": (\"Mathematical Functions\", \"FFT Computation\", 131072)\n",
    "        }\n",
    "        \n",
    "        if benchmark_type in quick_benchmarks:\n",
    "            category, name, size = quick_benchmarks[benchmark_type]\n",
    "            return run_selected_benchmark(category, name, size)\n",
    "        \n",
    "        return \"âŒ Quick benchmark not found\", \"Error\"\n",
    "\n",
    "    # Event handlers\n",
    "    def load_sample_code(sample_name):\n",
    "        if sample_name and sample_name in sample_codes:\n",
    "            return sample_codes[sample_name]\n",
    "        return \"\"\n",
    "    \n",
    "    # Wire up the interface\n",
    "    sample_dropdown.change(load_sample_code, inputs=[sample_dropdown], outputs=[code_input])\n",
    "    load_sample_btn.click(load_sample_code, inputs=[sample_dropdown], outputs=[code_input])\n",
    "    \n",
    "    submit_btn.click(\n",
    "        chat_with_mentor,\n",
    "        inputs=[message_input, code_input, chatbot],\n",
    "        outputs=[message_input, code_input, chatbot, gr.State(), gr.State()]\n",
    "    )\n",
    "    \n",
    "    clear_btn.click(clear_chat, outputs=[chatbot, gr.State(), gr.State()])\n",
    "    \n",
    "    analyze_btn.click(\n",
    "        analyze_code_only,\n",
    "        inputs=[analyze_code],\n",
    "        outputs=[analysis_results, optimized_code]\n",
    "    )\n",
    "    \n",
    "    generate_tutorial_btn.click(\n",
    "        get_tutorial,\n",
    "        inputs=[tutorial_topic],\n",
    "        outputs=[tutorial_content]\n",
    "    )\n",
    "    \n",
    "    summary_btn.click(\n",
    "        lambda: gpu_mentor.get_execution_summary(),\n",
    "        outputs=[execution_summary]\n",
    "    )\n",
    "    \n",
    "    # Benchmarking event handlers\n",
    "    benchmark_category.change(\n",
    "        update_benchmark_options,\n",
    "        inputs=[benchmark_category],\n",
    "        outputs=[benchmark_name, benchmark_size]\n",
    "    )\n",
    "    \n",
    "    benchmark_name.change(\n",
    "        update_size_options,\n",
    "        inputs=[benchmark_category, benchmark_name],\n",
    "        outputs=[benchmark_size]\n",
    "    )\n",
    "    \n",
    "    run_benchmark_btn.click(\n",
    "        run_selected_benchmark,\n",
    "        inputs=[benchmark_category, benchmark_name, benchmark_size],\n",
    "        outputs=[benchmark_results, benchmark_status]\n",
    "    )\n",
    "    \n",
    "    recent_benchmarks_btn.click(\n",
    "        show_recent_benchmarks,\n",
    "        outputs=[recent_benchmarks, recent_benchmarks]\n",
    "    )\n",
    "    \n",
    "    # Quick benchmark buttons\n",
    "    quick_matrix_btn.click(\n",
    "        lambda: run_quick_benchmark(\"Matrix Ops\"),\n",
    "        outputs=[benchmark_results, benchmark_status]\n",
    "    )\n",
    "    \n",
    "    quick_dataframe_btn.click(\n",
    "        lambda: run_quick_benchmark(\"DataFrame Ops\"),\n",
    "        outputs=[benchmark_results, benchmark_status]\n",
    "    )\n",
    "    \n",
    "    quick_ml_btn.click(\n",
    "        lambda: run_quick_benchmark(\"ML Algorithms\"),\n",
    "        outputs=[benchmark_results, benchmark_status]\n",
    "    )\n",
    "    \n",
    "    quick_math_btn.click(\n",
    "        lambda: run_quick_benchmark(\"Math Functions\"),\n",
    "        outputs=[benchmark_results, benchmark_status]\n",
    "    )\n",
    "\n",
    "# Launch the enhanced interface\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e095a615",
   "metadata": {},
   "source": [
    "## 16. Example Usage & Testing\n",
    "\n",
    "Let's test the GPU Mentor system with some example interactions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978a676d",
   "metadata": {},
   "source": [
    "## ðŸ Testing the Benchmarking System\n",
    "\n",
    "The new benchmarking feature provides comprehensive CPU vs GPU performance comparisons across multiple domains:\n",
    "\n",
    "### ðŸš€ New Features Added:\n",
    "\n",
    "1. **Interactive Benchmarking Tab** - Compare CPU vs GPU performance across different workloads\n",
    "2. **Educational Content** - Learn why GPU acceleration works for specific operations\n",
    "3. **Visual Performance Analysis** - See speedup charts and scaling insights\n",
    "4. **Code Examples** - Get optimized CPU and GPU implementations\n",
    "5. **Quick Benchmark Buttons** - Run common benchmarks with one click\n",
    "\n",
    "### ðŸ“Š Available Benchmark Categories:\n",
    "\n",
    "- **Matrix Operations**: NumPy vs CuPy for linear algebra\n",
    "- **DataFrame Operations**: Pandas vs cuDF for data processing  \n",
    "- **Machine Learning**: scikit-learn vs cuML for ML algorithms\n",
    "- **Mathematical Functions**: NumPy vs CuPy for numerical computing\n",
    "\n",
    "### ðŸŽ¯ Key Learning Objectives:\n",
    "\n",
    "- Understand when GPU acceleration provides significant benefits\n",
    "- Learn optimization techniques for different GPU libraries\n",
    "- Experience hands-on performance comparison\n",
    "- Discover scaling patterns with problem size\n",
    "- Explore memory and computational trade-offs\n",
    "\n",
    "### ðŸ’» How to Use:\n",
    "\n",
    "1. **Launch the Interface**: Run the Gradio interface cell\n",
    "2. **Navigate to Benchmarking Tab**: Click on \"ðŸ Performance Benchmarking\"\n",
    "3. **Select Parameters**: Choose category, benchmark, and problem size\n",
    "4. **Run Benchmark**: Click \"Run Benchmark\" to see CPU vs GPU comparison\n",
    "5. **Explore Results**: Review performance analysis, visualizations, and code examples\n",
    "6. **Try Quick Benchmarks**: Use the quick benchmark buttons for common operations\n",
    "\n",
    "The system will provide detailed explanations of why certain operations benefit from GPU acceleration and offer educational insights based on NVIDIA Rapids best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dcd7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the enhanced GPU Mentor with integrated LLM + code execution\n",
    "sample_numpy_code = \"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# Create large arrays\n",
    "n = 1000\n",
    "x = np.random.rand(n)\n",
    "y = np.random.rand(n)\n",
    "\n",
    "# Compute distance\n",
    "result = np.sqrt(x**2 + y**2)\n",
    "mean_distance = np.mean(result)\n",
    "\n",
    "print(f\"Array size: {n}\")\n",
    "print(f\"Mean distance: {mean_distance:.4f}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== Testing Enhanced GPU Mentor (LLM + Code Integration) ===\")\n",
    "\n",
    "# Test the integrated approach - LLM sees both question and code together\n",
    "user_question = \"How can I optimize this distance calculation for GPU acceleration?\"\n",
    "\n",
    "try:\n",
    "    # This now sends both the question AND code to the LLM together\n",
    "    response = gpu_mentor.process_user_input(user_question, sample_numpy_code)\n",
    "    \n",
    "    print(\"ðŸ¤– AI Response (with code context):\")\n",
    "    print(response[\"text_response\"][:300] + \"...\" if len(response[\"text_response\"]) > 300 else response[\"text_response\"])\n",
    "    \n",
    "    print(\"\\nâš¡ Code Execution Results:\")\n",
    "    if response[\"code_output\"]:\n",
    "        output = response[\"code_output\"]\n",
    "        print(f\"Status: {output.get('status', 'unknown')}\")\n",
    "        print(f\"Execution time: {output.get('execution_time', 0):.4f}s\")\n",
    "        print(f\"Output: {output.get('stdout', 'No output')}\")\n",
    "        print(f\"Variables: {list(output.get('variables', {}).keys())}\")\n",
    "    \n",
    "    print(\"\\nðŸ” Code Analysis:\")\n",
    "    if response[\"code_analysis\"]:\n",
    "        analysis = response[\"code_analysis\"]\n",
    "        print(f\"Libraries: {analysis.get('libraries_detected', [])}\")\n",
    "        print(f\"Speedup potential: {analysis.get('estimated_speedup', 1.0):.1f}x\")\n",
    "    \n",
    "    print(\"\\nðŸš€ Optimized Code:\")\n",
    "    if response[\"optimized_code\"]:\n",
    "        print(response[\"optimized_code\"][:200] + \"...\" if len(response[\"optimized_code\"]) > 200 else response[\"optimized_code\"])\n",
    "    \n",
    "    print(\"\\nðŸ¤” Socratic Questions:\")\n",
    "    for i, question in enumerate(response.get(\"socratic_questions\", []), 1):\n",
    "        print(f\"{i}. {question}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error testing enhanced mentor: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Testing Safe Code Execution...\")\n",
    "\n",
    "# Test safe code execution\n",
    "test_code = \"\"\"\n",
    "import numpy as np\n",
    "data = np.array([1, 2, 3, 4, 5])\n",
    "result = np.sum(data)\n",
    "print(f\"Sum: {result}\")\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    output = gpu_mentor._execute_code_safely(test_code)\n",
    "    print(f\"Execution status: {output['status']}\")\n",
    "    print(f\"Execution time: {output['execution_time']:.4f}s\")\n",
    "    print(f\"Output: {output['stdout']}\")\n",
    "    print(f\"Variables: {output['variables']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in safe execution: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39546133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the enhanced GPU Mentor (without actual Sol execution for demo)\n",
    "print(\"\\n=== Testing Enhanced GPU Mentor ===\")\n",
    "\n",
    "# Simulate a user interaction\n",
    "user_question = \"How can I accelerate matrix multiplication with CuPy?\"\n",
    "sample_code = \"\"\"\n",
    "import numpy as np\n",
    "A = np.random.rand(500, 500)\n",
    "B = np.random.rand(500, 500)\n",
    "C = np.dot(A, B)\n",
    "\"\"\"\n",
    "\n",
    "# Test just the RAG response and code analysis (skip actual benchmarking)\n",
    "try:\n",
    "    # Get RAG response\n",
    "    rag_result = gpu_mentor.rag_graph.invoke({\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": user_question}]\n",
    "    })\n",
    "    print(\"RAG Response:\", rag_result[\"messages\"][-1].content[:200] + \"...\")\n",
    "    \n",
    "    # Analyze code\n",
    "    analysis = gpu_mentor.code_optimizer.analyze_code(sample_code)\n",
    "    print(\"\\nCode Analysis:\", analysis)\n",
    "    \n",
    "    # Generate Socratic questions\n",
    "    questions = gpu_mentor._generate_socratic_questions(analysis, user_question)\n",
    "    print(\"\\nSocratic Questions:\")\n",
    "    for i, q in enumerate(questions, 1):\n",
    "        print(f\"{i}. {q}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error testing GPU Mentor: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai25.06",
   "language": "python",
   "name": "genai25.06"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
