{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4941058a-6c14-4330-8dee-a9e137c516f3",
   "metadata": {},
   "source": [
    "## **Notebook 1: Introduction to CuPy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f1a9a3-81de-419f-8e2b-e149e308edc9",
   "metadata": {},
   "source": [
    "This notebook will guide you through the basics of CuPy, demonstrating its capabilities in accelerating numerical computations and highlighting key concepts such as kernel overhead, data movement, and the use of streams and events.\n",
    "\n",
    "Throughout this tutorial, we'll explore:\n",
    "1. The similarities and differences between NumPy and CuPy\n",
    "2. Performance comparisons between CPU and GPU computations\n",
    "3. Techniques for optimizing GPU performance, including kernel fusion and managing data transfer\n",
    "4. Advanced features like CuPy backends and the use of streams for concurrent operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ccb886-42d0-465e-9ede-981f6ae4059f",
   "metadata": {},
   "source": [
    "--- \n",
    "**Introduction to CuPy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbcc303-3c50-45c6-82f1-e7068e6d0665",
   "metadata": {},
   "source": [
    "NumPy is a widely used library for numerical computing in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcf823dc-3fef-431f-9abb-ee827c0cf838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "size = 512\n",
    "\n",
    "A = np.random.randn(size, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2dfcf6",
   "metadata": {},
   "source": [
    "Below we are running a QR decomposition on our 2D ndarray. The magic function `timeit` lets us measure the execution time. It will show the average time it took to run over 5 executions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "658fd687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.1 ms ± 10.8 ms per loop (mean ± std. dev. of 7 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 5 Q, R = np.linalg.qr(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7dcb8c-7141-46f3-bd08-ea07261e9e65",
   "metadata": {},
   "source": [
    "CuPy uses a NumPy-like interface. Porting a NumPy code to CuPy can be as simple as changing your import statement. In this workshop, we'll always use `import cupy as cp` for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95d196d0-c618-4805-863a-3b57e873d807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "size = 512\n",
    "\n",
    "A = cp.random.randn(size, size)\n",
    "Q, R = cp.linalg.qr(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32b140a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.16 ms ± 91 μs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 5 Q, R = cp.linalg.qr(A) ; cp.cuda.Device().synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e954a9de-3519-400e-8a44-d59668bd26f2",
   "metadata": {},
   "source": [
    "We already see a substantial speedup with no real code changes! \n",
    "\n",
    "Notice the additional call to `cp.cuda.Device().synchronize()` in the CuPy version. GPU kernel calls are asynchronous with respect to the CPU. Our call to `synchronize()` ensures the GPU finishes to completion, so we can accurately measure  the elapsed time. We don't generally need to add these calls to production CuPy codes.\n",
    "\n",
    "NumPy is typically used to perform computations on fixed-size multidimensional _arrays_ of data. The data is stored in the `numpy.ndarray` object. CuPy implements a similar class called the `cupy.ndarray`. But while the `numpy.ndarray` data resides in host (CPU) memory, the contents of a `cupy.ndarray` persist in GPU memory. CuPy provides several helper functions to convert between CuPy and NumPy `ndarrays` - facilitating data transfer to/from the GPU device."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf42ada-78f9-4664-89ad-b292addd5285",
   "metadata": {},
   "source": [
    "Let's start by initializing our data on the host. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d45cd02d-7a0a-4e8e-9505-569b2cd464cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_cpu is a <class 'numpy.ndarray'>\n",
      "With initial values:\n",
      " [[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "A_cpu = np.array([[1, 2, 3], [4, 5, 6]], np.int32)\n",
    "\n",
    "print(\"A_cpu is a\", type(A_cpu))\n",
    "print(\"With initial values:\\n\", A_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feadd54-6118-465e-8a28-f23a21c6b236",
   "metadata": {},
   "source": [
    "Next, we copy the data from the host over to our GPU device. From here, we can call our CuPy `square` function on the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eca4b14c-fe6e-4ba7-82fc-0c50219e1358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_gpu is a <class 'cupy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "A_gpu = cp.asarray(A_cpu)\n",
    "print(\"A_gpu is a\", type(A_gpu))\n",
    "\n",
    "A_gpu = cp.square(A_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef292faf-23a8-4346-a721-fc22c2d2f6df",
   "metadata": {},
   "source": [
    "Lastly, we copy data back to the host, and can run our NumPy `square` function again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0db67a8-67d9-410f-a7e7-49f58161fa44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squared values:\n",
      " [[ 1  4  9]\n",
      " [16 25 36]]\n"
     ]
    }
   ],
   "source": [
    "A_cpu = cp.asnumpy(A_gpu)\n",
    "\n",
    "print(\"Squared values:\\n\", A_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3570d8dd-7f54-4d65-83b7-6cc38d6b4cc2",
   "metadata": {},
   "source": [
    "Note that NumPy and CuPy ndarrys are not implicitly convertible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7eacf74-5f07-42b0-9b6f-8af7a302d25a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,   16,   81],\n",
       "       [ 256,  625, 1296]], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Fix the code to run on either the CPU or GPU\n",
    "cp.square(A_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7841a1df",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click to Reveal Solution</summary>\n",
    "\n",
    "```python\n",
    "cp.square(A_gpu)\n",
    "# or\n",
    "np.square(A_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc84591b-2329-44a8-a266-baa30866086b",
   "metadata": {},
   "source": [
    "The GPU is a powerhouse of parallel computing performance, and can process math operations faster than the CPU. This is easy to see by comparing performance of CuPy vs NumPy, particularly for dense linear algebra operations. Let's look at a multiplication of 4096x4096 matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c64595b-a9b7-43c4-a5ed-87b579aca4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed wall clock time for NumPy = 1.10267 seconds.\n"
     ]
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "size = 4096\n",
    "\n",
    "start_time = perf_counter( )\n",
    "A_cpu = np.random.uniform(low=-1.0, high=1.0, size=(size,size) ).astype(np.float32)\n",
    "B_cpu = np.random.uniform(low=-1., high=1., size=(size,size) ).astype(np.float32)\n",
    "C_cpu = np.matmul(A_cpu,B_cpu)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print(\"Elapsed wall clock time for NumPy = %g seconds.\" % (stop_time - start_time) )\n",
    "\n",
    "del A_cpu\n",
    "del B_cpu\n",
    "del C_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821c8e20",
   "metadata": {},
   "source": [
    "With nearly identical code, we can run on the GPU using CuPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "865510a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed wall clock time for CuPy = 0.0157982 seconds.\n"
     ]
    }
   ],
   "source": [
    "A_gpu = cp.random.uniform(low=-1.0, high=1.0, size=(size,size) ).astype(cp.float32)\n",
    "B_gpu = cp.random.uniform(low=-1., high=1., size=(size,size) ).astype(cp.float32)\n",
    "C_gpu = cp.matmul(A_gpu,B_gpu) #Exclude one-time JIT overhead (more on this soon!)\n",
    "\n",
    "start_time = perf_counter( )\n",
    "C_gpu = cp.matmul(A_gpu,B_gpu)\n",
    "cp.cuda.Device(0).synchronize()\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print(\"Elapsed wall clock time for CuPy = %g seconds.\" % (stop_time - start_time) )\n",
    "\n",
    "del A_gpu\n",
    "del B_gpu\n",
    "del C_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d9b000-e8fa-4f67-a219-4640aea54abf",
   "metadata": {},
   "source": [
    "The GPU's strengths in computational throughput and memory bandwidth can lead to terrific application speedups. But we need to be considerate of two types of overhead when evaluating our problem for acceleration on the GPU with CuPy: kernel overhead, and data movement overhead.\n",
    "\n",
    "---\n",
    "\n",
    "**Kernel Overhead**\n",
    "\n",
    "CuPy compiles kernel codes on-the-fly using just-in-time (JIT) compilation. Therefore, there is a compilation overhead the first time a given function is called with CuPy. The compiled kernel code is cached, so compilation overhead is avoided for subsequent executions of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae1a0d2b-da82-4e46-999c-55bbad4c0517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5643\n",
      "0.0015\n",
      "0.0014\n",
      "0.0014\n",
      "0.0014\n"
     ]
    }
   ],
   "source": [
    "size = 512\n",
    "\n",
    "for _ in range(5):\n",
    "    A = cp.random.randn(size, size).astype(np.float32)\n",
    "    \n",
    "    start_time = perf_counter( )\n",
    "    cp.linalg.det(A)\n",
    "    cp.cuda.Device().synchronize()\n",
    "    stop_time = perf_counter( )\n",
    "    \n",
    "    print('%.4f' % (stop_time - start_time))\n",
    "    del A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827d8d9b-e4b2-4376-a441-b6bbf9a78af8",
   "metadata": {},
   "source": [
    "You may also notice a one-time overhead upon first calling a CuPy function in a program. This overhead is associated with the creation of a CUDA context by the CUDA driver, which happens the first time any CUDA API is invoked in a program.\n",
    "\n",
    "In addition, there is a CUDA kernel launch overhead that is penalized each time a GPU kernel is launched. The overhead is on the order of a few microseconds. For this reason, launching many small CUDA kernels in an application will generally lead to poor performance. The kernel launch overhead may dominate your runtime for very small problems, but for large datasets the overhead will be small compared to the actual GPU computation work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2061b6b0-169f-4e64-8556-b3fd86b97dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input Matrix size: 64 x 64 \n",
      "numpy 0.000345\n",
      "cupy 0.000381\n",
      "\n",
      "Input Matrix size: 128 x 128 \n",
      "numpy 0.002206\n",
      "cupy 0.000961\n",
      "\n",
      "Input Matrix size: 256 x 256 \n",
      "numpy 0.008502\n",
      "cupy 0.002157\n",
      "\n",
      "Input Matrix size: 512 x 512 \n",
      "numpy 0.054349\n",
      "cupy 0.005171\n",
      "\n",
      "Input Matrix size: 1024 x 1024 \n",
      "numpy 0.283332\n",
      "cupy 0.012422\n",
      "\n",
      "Input Matrix size: 2048 x 2048 \n",
      "numpy 1.970230\n",
      "cupy 0.032374\n"
     ]
    }
   ],
   "source": [
    "for size in [64, 128, 256, 512, 1024, 2048]:\n",
    "    print(\"\\nInput Matrix size: %d\" % size, \"x %d \" % size)\n",
    "    \n",
    "    for xp in [np, cp]:\n",
    "        A=xp.random.uniform(low=-1.0, high=1.0, size=(size,size)).astype(xp.float32)\n",
    "        xp.linalg.qr(A) #Exclude potential one-time JIT overhead\n",
    "        \n",
    "        start_time = perf_counter( )\n",
    "        xp.linalg.qr(A)\n",
    "        cp.cuda.Device().synchronize()\n",
    "        stop_time = perf_counter( )\n",
    "        \n",
    "        print(xp.__name__, '%f' % (stop_time - start_time))\n",
    "        del A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff335660-dfa6-4657-9489-7d31bb491d93",
   "metadata": {},
   "source": [
    "It's clear that increasing the problem size can help amortize the overhead of launching GPU kernels. Another common strategy is to merge multiple kernels together into a single combined kernel, reducing the total number of kernel launches in your program. CuPy supports kernel fusion in this manner via the `@cupy.fuse()` decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a7c0c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_diff(x, y):\n",
    "    return (x - y) * (x - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a08c4b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cp.fuse\n",
    "def fused_squared_diff(x, y):\n",
    "    return (x - y) * (x - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ef432c",
   "metadata": {},
   "source": [
    "The subtraction and multiplication operations are combined into a single kernel which improves performance by reducing intermediate memory usage and kernel launch overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "689b1c62-62f6-4da9-a3de-3e5afd5beda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squared_diff        :    CPU:    33.953 us   +/-  2.536 (min:    31.489 / max:    38.734) us     GPU-0:    39.424 us   +/-  3.584 (min:    35.840 / max:    47.104) us\n",
      "fused_squared_diff  :    CPU:    13.713 us   +/-  1.001 (min:    12.743 / max:    15.499) us     GPU-0:    18.432 us   +/-  1.832 (min:    16.384 / max:    22.528) us\n"
     ]
    }
   ],
   "source": [
    "size = 10000\n",
    "from cupyx.profiler import benchmark\n",
    "\n",
    "x = cp.arange(size)\n",
    "y = cp.arange(size)[::-1]\n",
    "\n",
    "print(benchmark(squared_diff, (x,y), n_repeat=10))\n",
    "print(benchmark(fused_squared_diff, (x,y), n_repeat=10))\n",
    "\n",
    "del x\n",
    "del y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1f4b7b",
   "metadata": {},
   "source": [
    "Note above we are using a new benchmarking technique. `Benchmark` is built into CuPy and is a useful tool for timing the elapsed time of a function running on both the CPU and GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52844f76",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279e909e",
   "metadata": {},
   "source": [
    "**Data Movement Overhead**\n",
    "\n",
    "The FLOP rate and memory bandwidth of a GPU can process data much more quickly than it can be fed with data over the PCIe bus. This problem is being tackled with novel interconnect technologies like NVLink. But it's a real imbalance we have to deal with for now.\n",
    "Let's look at an example where we initialize our input data GPU and then computes the dot product. Note that the result of the multiplication, the C matrix, is available on the GPU in case we need it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ebb0179-8137-4827-8eb2-2c960967593c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0\n",
      "NumPy = 1.79916 seconds\n",
      "CuPy = 0.00848454 seconds\n",
      "Speedup = 212.05\n",
      "Iteration  1\n",
      "NumPy = 1.80263 seconds\n",
      "CuPy = 0.00845099 seconds\n",
      "Speedup = 213.30\n",
      "Iteration  2\n",
      "NumPy = 1.79947 seconds\n",
      "CuPy = 0.00828861 seconds\n",
      "Speedup = 217.10\n"
     ]
    }
   ],
   "source": [
    "size = int(1e8)\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"Iteration \", i)\n",
    "    \n",
    "    start_time = perf_counter( )\n",
    "    A_cpu=np.random.rand(size).astype(np.float32)\n",
    "    B_cpu=np.random.rand(size).astype(np.float32)\n",
    "    C_cpu = np.dot(A_cpu,B_cpu)\n",
    "    stop_time = perf_counter( )\n",
    "    cpu_time = stop_time - start_time\n",
    "    \n",
    "    print(\"NumPy = %g seconds\" % cpu_time )\n",
    "\n",
    "    del A_cpu\n",
    "    del B_cpu\n",
    "    del C_cpu\n",
    "\n",
    "    start_time = perf_counter( )\n",
    "    A_gpu=cp.random.rand(size).astype(cp.float32)\n",
    "    B_gpu=cp.random.rand(size).astype(cp.float32)\n",
    "    C_gpu = cp.dot(A_gpu,B_gpu)\n",
    "    cp.cuda.Device(0).synchronize()\n",
    "    stop_time = perf_counter( )\n",
    "    \n",
    "    gpu_time = stop_time - start_time\n",
    "    \n",
    "    print(\"CuPy = %g seconds\" % gpu_time )\n",
    "    print(\"Speedup = %.2f\" % (cpu_time/gpu_time))\n",
    "\n",
    "    del A_gpu\n",
    "    del B_gpu\n",
    "    del C_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087ea831-f9e3-41c1-888d-8fed77d9fd1f",
   "metadata": {},
   "source": [
    "But what if the input data for the `dot` operation resides in the system memory? We need to move the data over the PCIe bus (from the host to the GPU) using `cp.asarray()`. \n",
    "\n",
    "Modify the following cell to initialize the ndarray data with NumPy. \n",
    "\n",
    "How does the speedup change after the additional cost of data movement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1be63e60-fb09-4936-886d-7e91aebacd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0\n",
      "NumPy = 1.79835 seconds\n",
      "CuPy = 1.76958 seconds\n",
      "Speedup = 1.02\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'A_gpu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mCuPy = \u001b[39m\u001b[38;5;132;01m%g\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m'\u001b[39m % gpu_time )\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSpeedup = \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[33m\"\u001b[39m % (cpu_time/gpu_time))\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[43mA_gpu\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m B_gpu\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m C_gpu\n",
      "\u001b[31mNameError\u001b[39m: name 'A_gpu' is not defined"
     ]
    }
   ],
   "source": [
    "size = int(1e8)\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"Iteration \", i)\n",
    "    start_time = perf_counter( )\n",
    "    A_cpu=np.random.rand(size).astype(np.float32)\n",
    "    B_cpu=np.random.rand(size).astype(np.float32)\n",
    "    C_cpu = np.dot(A_cpu,B_cpu)\n",
    "    stop_time = perf_counter( )\n",
    "    cpu_time = stop_time - start_time\n",
    "    \n",
    "    print(\"NumPy = %g seconds\" % cpu_time )\n",
    "\n",
    "    del A_cpu\n",
    "    del B_cpu\n",
    "    del C_cpu\n",
    "    \n",
    "    start_time = perf_counter( )\n",
    "    A_cpu=np.random.rand(size).astype(np.float32)\n",
    "    B_cpu=np.random.rand(size).astype(np.float32)\n",
    "\n",
    "    # TODO: Insert CuPy code here\n",
    "    A_gpu=cp.asarray(A_cpu)\n",
    "    B_gpu=cp.asarray(B_cpu)\n",
    "    C_gpu = cp.dot(A_gpu,B_gpu)\n",
    "    cp.cuda.Device(0).synchronize()\n",
    "    stop_time = perf_counter( )\n",
    "    gpu_time = stop_time - start_time\n",
    "    \n",
    "    print('CuPy = %g seconds' % gpu_time )\n",
    "    print(\"Speedup = %.2f\" % (cpu_time/gpu_time))\n",
    "\n",
    "    del A_gpu\n",
    "    del B_gpu\n",
    "    del C_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c8e020",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click to Reveal Solution</summary>\n",
    "\n",
    "```python\n",
    "size = int(1e8)\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"Iteration \", i)\n",
    "    \n",
    "    start_time = perf_counter( )\n",
    "    \n",
    "    A_cpu=np.random.rand(size).astype(np.float32)\n",
    "    B_cpu=np.random.rand(size).astype(np.float32)\n",
    "    \n",
    "    A_gpu=cp.asarray(A_cpu)\n",
    "    B_gpu=cp.asarray(B_cpu)\n",
    "    C_gpu = cp.dot(A_gpu,B_gpu)\n",
    "    cp.cuda.Device(0).synchronize()\n",
    "    \n",
    "    stop_time = perf_counter( )\n",
    "    gpu_time = stop_time - start_time\n",
    "    \n",
    "    print('cupy = %g seconds' % gpu_time )\n",
    "    print(\"Speedup = %.2f\" % (cpu_time/gpu_time))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e71efa8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bbb837",
   "metadata": {},
   "source": [
    "**Streams and Events**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055b8783",
   "metadata": {},
   "source": [
    "Our final CuPy exploration will introduce how streams can be utilized. A stream represents a sequence of operations that are executed in order on the GPU. By default, operations are asynchronous and running on the default stream. We can check the current stream we are using by running the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dfe15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.cuda.get_current_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61cdf6d",
   "metadata": {},
   "source": [
    "Now let's look at CuPy code that performs two matrix inversions sequentially. We have seen similar code already, this is all happening on a single stream. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2d525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 6000\n",
    "\n",
    "A_gpu = cp.random.rand(size, size).astype(cp.float32)\n",
    "\n",
    "start_time= perf_counter()\n",
    "C1 = cp.linalg.inv(A_gpu)\n",
    "C2 = cp.linalg.inv(A_gpu)\n",
    "cp.cuda.Stream.null.synchronize()\n",
    "stop_time = perf_counter()\n",
    "\n",
    "print(f\"Baseline implementation time: {stop_time - start_time:.4f} seconds\")\n",
    "\n",
    "del C1\n",
    "del C2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59796029",
   "metadata": {},
   "source": [
    "We can explicitly create multiple streams to make operations happen concurrently. This can reduce the overall execution time and have more fine grained control over the program. In the below example, `stream1` and `stream2` run concurrently. We also utilize events, which we can use to indicate teh completion of an operation within a specific stream. This is the underlying functionality to the `benchmark` function that we used earlier! This allows us to check when specific operations on the GPU have completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c2cde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = perf_counter()\n",
    "\n",
    "stream1 = cp.cuda.Stream()\n",
    "stream2 = cp.cuda.Stream()\n",
    "\n",
    "event1 = cp.cuda.Event()\n",
    "event2 = cp.cuda.Event()\n",
    "\n",
    "with stream1:\n",
    "    C1 = cp.linalg.inv(A_gpu)\n",
    "    event1.record(stream1)\n",
    "\n",
    "with stream2:\n",
    "    C2 = cp.linalg.inv(A_gpu)\n",
    "    event2.record(stream2)\n",
    "\n",
    "event1.synchronize()\n",
    "event2.synchronize()\n",
    "\n",
    "stop_time = perf_counter()\n",
    "print(f\"Stream and event time: {stop_time - start_time:.4f} seconds\")\n",
    "\n",
    "del C1\n",
    "del C2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82076ac4-4ec3-4913-a891-87f029760155",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Please restart the kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cbe283-6890-4758-b8b6-9d5f6d3fdfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ab1725-802b-4156-821a-100bc5768fb9",
   "metadata": {},
   "source": [
    "In this notebook, we've explored the fundamentals of CuPy and its potential for accelerating numerical computations in Python. We've seen how CuPy can provide significant speedups over NumPy for large-scale operations, particularly in linear algebra and matrix computations. Key takeaways include:\n",
    "\n",
    "1. CuPy offers a familiar NumPy-like interface, making it easy to port existing code to GPU acceleration.\n",
    "2. Performance gains can be substantial, especially for large datasets and complex operations.\n",
    "3. It's crucial to consider both kernel overhead and data movement when optimizing GPU performance.\n",
    "4. Advanced features like kernel fusion, cuTENSOR backends, and multi-stream operations can further enhance performance.\n",
    "5. Understanding the nuances of GPU programming, such as asynchronous execution and memory management, is essential for effective use of CuPy.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids25.02",
   "language": "python",
   "name": "rapids25.02"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
